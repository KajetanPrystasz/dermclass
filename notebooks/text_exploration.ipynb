{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:24:26,791 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class lichen_planus\n",
      "2020-12-06 15:24:26,802 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class pityriasis_rosea\n",
      "2020-12-06 15:24:26,813 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class psoriasis\n",
      "2020-12-06 15:24:26,815 — dermclass_models2.preprocessing — INFO —_load_structured_data:277 — Successfully loaded the data from file\n",
      "2020-12-06 15:24:26,816 — dermclass_models2.preprocessing — INFO —_split_target_structured:57 — Successfully splat the target\n",
      "2020-12-06 15:24:26,818 — dermclass_models2.preprocessing — INFO —_split_train_test_structured:80 — Successfully splat train and test data\n",
      "2020-12-06 15:24:26,819 — dermclass_models2.preprocessing — INFO —_load_data_structured:87 — Successfully loaded the data\n",
      "2020-12-06 15:24:27,355 — dermclass_models2.pipeline — INFO —get_processing_pipeline:461 — Successfully loaded processing pipeline\n",
      "2020-12-06 15:24:27,885 — dermclass_models2.pipeline — INFO —get_processing_pipeline:461 — Successfully loaded processing pipeline\n",
      "2020-12-06 15:24:35,373 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:144 — Finding hyperparameters for MultinomialNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-06 15:24:35,373]\u001b[0m A new study created in memory with name: MultinomialNB\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:35,431]\u001b[0m Trial 1 finished with value: 0.8888888888888888 and parameters: {'alpha': 0.6}. Best is trial 1 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:35,510]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'alpha': 2.6}. Best is trial 1 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:35,510]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'alpha': 4.8}. Best is trial 1 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:35,669]\u001b[0m Trial 3 finished with value: 0.8888888888888888 and parameters: {'alpha': 0.6}. Best is trial 1 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:35,768]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'alpha': 5.0}. Best is trial 1 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:35,771]\u001b[0m Trial 7 finished with value: 1.0 and parameters: {'alpha': 0.1}. Best is trial 7 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:35,850]\u001b[0m Trial 6 finished with value: 0.7777777777777778 and parameters: {'alpha': 1.2000000000000002}. Best is trial 1 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:36,008]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'alpha': 4.6}. Best is trial 7 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:36,107]\u001b[0m Trial 8 finished with value: 0.8888888888888888 and parameters: {'alpha': 0.6}. Best is trial 7 with value: 1.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:24:36,191]\u001b[0m Trial 9 finished with value: 0.8888888888888888 and parameters: {'alpha': 1.1}. Best is trial 7 with value: 1.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:24:36,291 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:161 — Best params found for: MultinomialNB with score: 1.0\n",
      "2020-12-06 15:24:36,291 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:166 — Successfully tuned hyperparameters\n",
      "2020-12-06 15:24:36,291 — dermclass_models2.pipeline — WARNING —get_model:447 — Warning! get_model for transformers function in TextPipeline returns unfitted model\n",
      "2020-12-06 15:24:39,420 — dermclass_models2.pipeline — INFO —get_modeling_pipeline:489 — Successfully loaded modeling pipeline\n",
      "Found 22 files belonging to 3 classes.\n",
      "Using 18 files for training.\n",
      "Found 22 files belonging to 3 classes.\n",
      "Using 4 files for validation.\n",
      "2020-12-06 15:24:40,988 — dermclass_models2.preprocessing — INFO —_load_dataset:301 — Successfully loaded train and validation datasets \n",
      "2020-12-06 15:24:41,000 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:120 — Successfully prefetched train, test and validation datasets\n",
      "2020-12-06 15:24:41,001 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:123 — Number of train batches: 6        Number of validation batches: 1        Number of test batches: 1\n",
      "2020-12-06 15:24:41,916 — dermclass_models2.pipeline — INFO —get_processing_pipeline:461 — Successfully loaded processing pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:24:43,614 — dermclass_models2.pipeline — WARNING —get_model:447 — Warning! get_model for transformers function in TextPipeline returns unfitted model\n",
      "2020-12-06 15:24:43,623 — dermclass_models2.pipeline — INFO —_compile_model:206 — Successfully compiled tf model\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 4s 584ms/step - loss: 1.1832 - accuracy: 0.3333 - val_loss: 1.4454 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 1.3810 - accuracy: 0.2778 - val_loss: 0.5015 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 1.6147 - accuracy: 0.2222 - val_loss: 1.1202 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 1.1000 - accuracy: 0.3333 - val_loss: 0.8910 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 1.1551 - accuracy: 0.3889 - val_loss: 0.9841 - val_accuracy: 1.0000\n",
      "2020-12-06 15:25:05,867 — dermclass_models2.pipeline — INFO —_train_model:230 — Successfully trained tf model\n",
      "2020-12-06 15:25:05,867 — dermclass_models2.pipeline — INFO —get_modeling_pipeline:489 — Successfully loaded modeling pipeline\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.0458 - accuracy: 0.3333\n",
      "2020-12-06 15:25:07,461 — dermclass_models2.pipeline — INFO —get_best_modeling_pipeline_type:436 — Successfully found best modeling pipeline type\n",
      "2020-12-06 15:25:07,492 — dermclass_models2.persistence — INFO —remove_old_pipelines:63 — C:\\Users\\Kajetan\\Desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\pickles\\text_pipeline_0.3.0 removed\n",
      "2020-12-06 15:25:09,204 — dermclass_models2.persistence — INFO —save_pipeline:83 — Saved pipeline Pipeline(steps=[('Processing pipeline',\n",
      "                 Pipeline(steps=[('Lemmatization, punctuation and stopwords '\n",
      "                                  'removal',\n",
      "                                  SpacyPreprocessor()),\n",
      "                                 ('Tfidf Vectorizer',\n",
      "                                  TfidfVectorizer(ngram_range=(1, 2)))])),\n",
      "                ('Model', MultinomialNB(alpha=0.1))]), to path C:\\Users\\Kajetan\\Desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\pickles\\text_pipeline_0.3.0\n"
     ]
    }
   ],
   "source": [
    "from dermclass_models2.train_pipeline import TextTrainPipeline\n",
    "ttp = TextTrainPipeline()\n",
    "ttp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dermclass_models2.train_pipeline import StructuredTrainPipeline\n",
    "stp = StructuredTrainPipeline()\n",
    "stp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 14:45:11,788 — dermclass_models2.preprocessing — INFO —_get_avg_img_size:175 — Mean height is: 473, mean width is: 561\n",
      "2020-12-06 14:45:11,788 — dermclass_models2.preprocessing — INFO —_get_efficientnet_and_size:197 — Chosen model is <function EfficientNetB6 at 0x000001491EA2EDC8> with img_size (528, 528)\n",
      "Found 71 files belonging to 3 classes.\n",
      "Using 57 files for training.\n",
      "Found 71 files belonging to 3 classes.\n",
      "Using 14 files for validation.\n",
      "2020-12-06 14:45:13,459 — dermclass_models2.preprocessing — INFO —_load_dataset:225 — Successfully loaded train and validation datasets \n",
      "2020-12-06 14:45:13,462 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:120 — Successfully prefetched train, test and validation datasets\n",
      "2020-12-06 14:45:13,463 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:123 — Number of train batches: 19        Number of validation batches: 3        Number of test batches: 2\n",
      "2020-12-06 14:45:13,464 — dermclass_models2.pipeline — INFO —set_img_size_and_model_obj:306 — Successfully set img size and model obj\n",
      "2020-12-06 14:45:13,486 — dermclass_models2.pipeline — INFO —get_processing_pipeline:321 — Successfully loaded image processing pipeline\n",
      "2020-12-06 14:45:20,239 — dermclass_models2.pipeline — WARNING —get_model:331 — Warning! get_model function in ImagePipeline returns unfitted model\n",
      "2020-12-06 14:45:22,350 — dermclass_models2.pipeline — INFO —_compile_model:206 — Successfully compiled tf model\n",
      "Epoch 1/5\n",
      "19/19 [==============================] - 10s 536ms/step - loss: 1.0331 - accuracy: 0.5263 - val_loss: 0.8122 - val_accuracy: 0.6250\n",
      "Epoch 2/5\n",
      "19/19 [==============================] - 6s 293ms/step - loss: 0.8400 - accuracy: 0.6140 - val_loss: 0.7659 - val_accuracy: 0.8750\n",
      "Epoch 3/5\n",
      "19/19 [==============================] - 6s 293ms/step - loss: 0.7661 - accuracy: 0.6667 - val_loss: 0.6350 - val_accuracy: 0.8750\n",
      "Epoch 4/5\n",
      "19/19 [==============================] - 6s 294ms/step - loss: 0.6537 - accuracy: 0.7719 - val_loss: 0.5540 - val_accuracy: 0.8750\n",
      "Epoch 5/5\n",
      "19/19 [==============================] - 6s 293ms/step - loss: 0.6102 - accuracy: 0.7719 - val_loss: 0.4990 - val_accuracy: 0.8750\n",
      "2020-12-06 14:46:13,627 — dermclass_models2.pipeline — INFO —_train_model:230 — Successfully trained tf model\n",
      "2020-12-06 14:46:13,641 — dermclass_models2.pipeline — INFO —get_modeling_pipeline:353 — Successfully loaded modeling pipeline\n",
      "2020-12-06 14:46:13,653 — dermclass_models2.persistence — INFO —remove_old_pipelines:63 — C:\\Users\\Kajetan\\Desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\pickles\\image_pipeline_0.3.0 removed\n",
      "WARNING:tensorflow:From C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Kajetan\\Desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\pickles\\image_pipeline_0.3.0\\assets\n",
      "2020-12-06 14:48:43,502 — dermclass_models2.persistence — INFO —save_pipeline:85 — Saved pipeline <tensorflow.python.keras.engine.sequential.Sequential object at 0x0000014A0F10E108>, to path C:\\Users\\Kajetan\\Desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\pickles\\image_pipeline_0.3.0\n"
     ]
    }
   ],
   "source": [
    "from dermclass_models2.train_pipeline import ImageTrainPipeline\n",
    "itp = ImageTrainPipeline()\n",
    "itp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:07:35,878 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class lichen_planus\n",
      "2020-12-06 15:07:35,888 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class pityriasis_rosea\n",
      "2020-12-06 15:07:35,900 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class psoriasis\n",
      "2020-12-06 15:07:35,902 — dermclass_models2.preprocessing — INFO —_load_structured_data:277 — Successfully loaded the data from file\n",
      "2020-12-06 15:07:35,903 — dermclass_models2.preprocessing — INFO —_split_target_structured:57 — Successfully splat the target\n",
      "2020-12-06 15:07:35,904 — dermclass_models2.preprocessing — INFO —_split_train_test_structured:80 — Successfully splat train and test data\n",
      "2020-12-06 15:07:35,905 — dermclass_models2.preprocessing — INFO —_load_data_structured:87 — Successfully loaded the data\n",
      "2020-12-06 15:07:36,700 — dermclass_models2.pipeline — INFO —get_processing_pipeline:460 — Successfully loaded processing pipeline\n",
      "2020-12-06 15:07:37,200 — dermclass_models2.pipeline — INFO —get_processing_pipeline:460 — Successfully loaded processing pipeline\n",
      "2020-12-06 15:07:44,661 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:144 — Finding hyperparameters for MultinomialNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-06 15:07:44,663]\u001b[0m A new study created in memory with name: MultinomialNB\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:44,720]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'alpha': 3.0000000000000004}. Best is trial 1 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:44,953]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'alpha': 2.4000000000000004}. Best is trial 1 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:44,959]\u001b[0m Trial 2 finished with value: 0.8888888888888888 and parameters: {'alpha': 0.7000000000000001}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:45,185]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'alpha': 4.2}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:45,188]\u001b[0m Trial 3 finished with value: 0.0 and parameters: {'alpha': 3.9000000000000004}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:45,433]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'alpha': 1.8000000000000003}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:45,669]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'alpha': 3.3000000000000003}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:45,670]\u001b[0m Trial 5 finished with value: 0.7777777777777778 and parameters: {'alpha': 1.2000000000000002}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:46,626]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'alpha': 1.6}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:07:46,861]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'alpha': 3.2}. Best is trial 2 with value: 0.8888888888888888.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:07:47,199 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:161 — Best params found for: MultinomialNB with score: 0.8888888888888888\n",
      "2020-12-06 15:07:47,203 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:166 — Successfully tuned hyperparameters\n",
      "2020-12-06 15:07:47,204 — dermclass_models2.pipeline — WARNING —get_model:446 — Warning! get_model for transformers function in TextPipeline returns unfitted model\n",
      "2020-12-06 15:07:47,205 — dermclass_models2.pipeline — INFO —get_modeling_pipeline:486 — Successfully loaded modeling pipeline\n",
      "Found 22 files belonging to 3 classes.\n",
      "Using 18 files for training.\n",
      "Found 22 files belonging to 3 classes.\n",
      "Using 4 files for validation.\n",
      "2020-12-06 15:07:47,472 — dermclass_models2.preprocessing — INFO —_load_dataset:301 — Successfully loaded train and validation datasets \n",
      "2020-12-06 15:07:47,479 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:120 — Successfully prefetched train, test and validation datasets\n",
      "2020-12-06 15:07:47,479 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:123 — Number of train batches: 6        Number of validation batches: 1        Number of test batches: 1\n",
      "2020-12-06 15:07:48,420 — dermclass_models2.pipeline — INFO —get_processing_pipeline:460 — Successfully loaded processing pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:07:49,862 — dermclass_models2.pipeline — WARNING —get_model:446 — Warning! get_model for transformers function in TextPipeline returns unfitted model\n",
      "2020-12-06 15:07:49,870 — dermclass_models2.pipeline — INFO —_compile_model:206 — Successfully compiled tf model\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[3,512,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/transpose_3 (defined at C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:228) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/tf_distil_bert_for_sequence_classification_2/distilbert/embeddings/position_embeddings/embedding_lookup/Reshape/_284]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[3,512,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/transpose_3 (defined at C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:228) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_32151]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/transpose_3:\n tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/MatMul_1 (defined at C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:249)\n\nInput Source operations connected to node tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/transpose_3:\n tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/MatMul_1 (defined at C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:249)\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2d1251567dc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdermclass_models2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_pipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextTrainPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mttp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextTrainPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\kajetan\\desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\train_pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_datasets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mtransformers_modeling_pipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_modeling_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_sklearn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         modeling_pipeline = self.pipeline2.get_best_modeling_pipeline_type(transformers_modeling_pipeline,\n",
      "\u001b[1;32mc:\\users\\kajetan\\desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\pipeline.py\u001b[0m in \u001b[0;36mget_modeling_pipeline\u001b[1;34m(self, use_sklearn, x_train, x_test, y_train, y_test, train_dataset, validation_dataset, learning_rate, metrics, n_epochs)\u001b[0m\n\u001b[0;32m    479\u001b[0m             train_dataset, validation_dataset = (processing_pipeline(train_dataset),\n\u001b[0;32m    480\u001b[0m                                                  processing_pipeline(validation_dataset))\n\u001b[1;32m--> 481\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m             modeling_pipeline = _TransformersModelingPipeline(model=model,\n\u001b[0;32m    483\u001b[0m                                                               processing_pipeline=processing_pipeline)\n",
      "\u001b[1;32mc:\\users\\kajetan\\desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\pipeline.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, model, train_dataset, validation_dataset, n_epochs)\u001b[0m\n\u001b[0;32m    220\u001b[0m             model.fit(train_dataset.batch(self.config.BATCH_SIZE),\n\u001b[0;32m    221\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m                       epochs=n_epochs)\n\u001b[0m\u001b[0;32m    223\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             model.fit(train_dataset,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[3,512,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/transpose_3 (defined at C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:228) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/tf_distil_bert_for_sequence_classification_2/distilbert/embeddings/position_embeddings/embedding_lookup/Reshape/_284]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[3,512,12,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/transpose_3 (defined at C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:228) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_32151]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/transpose_3:\n tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/MatMul_1 (defined at C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:249)\n\nInput Source operations connected to node tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/transpose_3:\n tf_distil_bert_for_sequence_classification_2/distilbert/transformer/layer_._4/attention/MatMul_1 (defined at C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:249)\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "from dermclass_models2.train_pipeline import TextTrainPipeline\n",
    "ttp = TextTrainPipeline()\n",
    "ttp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFDistilBertForSequenceClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 files belonging to 3 classes.\n",
      "Using 18 files for training.\n",
      "Found 22 files belonging to 3 classes.\n",
      "Using 4 files for validation.\n",
      "2020-12-06 15:17:35,663 — dermclass_models2.preprocessing — INFO —_load_dataset:301 — Successfully loaded train and validation datasets \n",
      "2020-12-06 15:17:35,667 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:120 — Successfully prefetched train, test and validation datasets\n",
      "2020-12-06 15:17:35,668 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:123 — Number of train batches: 6        Number of validation batches: 1        Number of test batches: 1\n",
      "2020-12-06 15:17:35,675 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class lichen_planus\n",
      "2020-12-06 15:17:35,691 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class pityriasis_rosea\n",
      "2020-12-06 15:17:35,703 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class psoriasis\n",
      "2020-12-06 15:17:35,705 — dermclass_models2.preprocessing — INFO —_load_structured_data:277 — Successfully loaded the data from file\n",
      "2020-12-06 15:17:35,707 — dermclass_models2.preprocessing — INFO —_split_target_structured:57 — Successfully splat the target\n",
      "2020-12-06 15:17:35,709 — dermclass_models2.preprocessing — INFO —_split_train_test_structured:80 — Successfully splat train and test data\n",
      "2020-12-06 15:17:35,710 — dermclass_models2.preprocessing — INFO —_load_data_structured:87 — Successfully loaded the data\n"
     ]
    }
   ],
   "source": [
    "from dermclass_models2.pipeline import TextPipeline\n",
    "from dermclass_models2.preprocessing import TextPreprocessors\n",
    "tp = TextPreprocessors()\n",
    "tm = TextPipeline()\n",
    "train_dataset, validation_dataset, test_dataset = tp.load_data(get_datasets=True)\n",
    "x_train, x_test, y_train, y_test = tp.load_data(get_datasets=False)\n",
    "tm.fit_structured_data(x_train, x_test, y_train, y_test)\n",
    "tm.fit_datasets(train_dataset, validation_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:05:54,400 — dermclass_models2.pipeline — WARNING —get_model:446 — Warning! get_model for transformers function in TextPipeline returns unfitted model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification at 0x2f495bca7c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.get_model(use_sklearn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:05:55,264 — dermclass_models2.pipeline — INFO —get_processing_pipeline:460 — Successfully loaded processing pipeline\n"
     ]
    }
   ],
   "source": [
    "pppp = tm.get_processing_pipeline(use_sklearn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (512,), attention_mask: (512,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pppp(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:17:38,245 — dermclass_models2.pipeline — INFO —get_processing_pipeline:461 — Successfully loaded processing pipeline\n",
      "2020-12-06 15:17:38,695 — dermclass_models2.pipeline — INFO —get_processing_pipeline:461 — Successfully loaded processing pipeline\n",
      "2020-12-06 15:17:46,038 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:144 — Finding hyperparameters for MultinomialNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-06 15:17:46,038]\u001b[0m A new study created in memory with name: MultinomialNB\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:46,112]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'alpha': 2.5000000000000004}. Best is trial 2 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:46,115]\u001b[0m Trial 0 finished with value: 0.8888888888888888 and parameters: {'alpha': 0.6}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:46,353]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'alpha': 2.2}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:46,357]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'alpha': 3.3000000000000003}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:46,358]\u001b[0m Trial 1 finished with value: 0.8888888888888888 and parameters: {'alpha': 1.1}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:46,593]\u001b[0m Trial 5 finished with value: 0.0 and parameters: {'alpha': 3.6}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:46,826]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'alpha': 3.1}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:47,066]\u001b[0m Trial 3 finished with value: 0.8888888888888888 and parameters: {'alpha': 0.30000000000000004}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:47,994]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'alpha': 2.4000000000000004}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 15:17:48,241]\u001b[0m Trial 9 finished with value: 0.0 and parameters: {'alpha': 1.5000000000000002}. Best is trial 0 with value: 0.8888888888888888.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:17:48,598 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:161 — Best params found for: MultinomialNB with score: 0.8888888888888888\n",
      "2020-12-06 15:17:48,598 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:166 — Successfully tuned hyperparameters\n",
      "2020-12-06 15:17:48,610 — dermclass_models2.pipeline — WARNING —get_model:447 — Warning! get_model for transformers function in TextPipeline returns unfitted model\n",
      "2020-12-06 15:17:48,612 — dermclass_models2.pipeline — INFO —get_modeling_pipeline:488 — Successfully loaded modeling pipeline\n"
     ]
    }
   ],
   "source": [
    "sklearn_modeling_pipeline = tm.get_modeling_pipeline(use_sklearn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Processing pipeline',\n",
       "                 Pipeline(steps=[('Lemmatization, punctuation and stopwords '\n",
       "                                  'removal',\n",
       "                                  SpacyPreprocessor()),\n",
       "                                 ('Tfidf Vectorizer',\n",
       "                                  TfidfVectorizer(ngram_range=(1, 2)))])),\n",
       "                ('Model', MultinomialNB(alpha=0.6))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_modeling_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sklearn_modeling_pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lichen_planus', 'pityriasis_rosea', 'pityriasis_rosea',\n",
       "       'lichen_planus', 'psoriasis'], dtype='<U16')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        lichen_planus\n",
       "13    pityriasis_rosea\n",
       "8     pityriasis_rosea\n",
       "1        lichen_planus\n",
       "15           psoriasis\n",
       "Name: target, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:06:15,713 — dermclass_models2.pipeline — INFO —get_processing_pipeline:460 — Successfully loaded processing pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_39', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 15:06:17,142 — dermclass_models2.pipeline — WARNING —get_model:446 — Warning! get_model for transformers function in TextPipeline returns unfitted model\n",
      "2020-12-06 15:06:17,151 — dermclass_models2.pipeline — INFO —_compile_model:206 — Successfully compiled tf model\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 4s 590ms/step - loss: 1.2502 - accuracy: 0.2778 - val_loss: 1.0342 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 351ms/step - loss: 1.2921 - accuracy: 0.3333 - val_loss: 0.6524 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 1.1564 - accuracy: 0.2222 - val_loss: 0.7326 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 1.6184 - accuracy: 0.3333 - val_loss: 0.8592 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 328ms/step - loss: 1.1886 - accuracy: 0.2778 - val_loss: 1.0217 - val_accuracy: 1.0000\n",
      "2020-12-06 15:06:39,723 — dermclass_models2.pipeline — INFO —_train_model:230 — Successfully trained tf model\n",
      "2020-12-06 15:06:39,723 — dermclass_models2.pipeline — INFO —get_modeling_pipeline:486 — Successfully loaded modeling pipeline\n"
     ]
    }
   ],
   "source": [
    "transformers_modeling_pipeline = tm.get_modeling_pipeline(use_sklearn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dermclass_models2.pipeline import TextPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 12:43:04,915 — dermclass_models2.pipeline — INFO —get_processing_pipeline:430 — Successfully loaded processing pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 12:43:06,482 — dermclass_models2.pipeline — WARNING —get_model:416 — Warning! get_model for transformers function in TextPipeline returns unfitted model\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset or tm.train_dataset\n",
    "validation_dataset = validation_dataset or tm.validation_dataset\n",
    "\n",
    "processing_pipeline = tm.get_processing_pipeline(False)\n",
    "model = tm.get_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tm._compile_model(model, learning_rate, [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification at 0x1b230170288>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processing_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a4718c7118a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_dataset, validation_dataset = (processing_pipeline(train_dataset, tm.tokenizer),\n\u001b[0m\u001b[0;32m      2\u001b[0m                                      processing_pipeline(validation_dataset, tm.tokenizer))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processing_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset = (processing_pipeline(train_dataset, tm.tokenizer),\n",
    "                                     processing_pipeline(validation_dataset, tm.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': array([  101, 16770,  1024,  1013,  1013,  4372,  1012, 16948,  1012,\n",
      "        8917,  1013, 15536,  3211,  1013,  5622,  8661,  1035,  2933,\n",
      "        2271,  5622,  8661,  2933,  2271,  1006,  6948,  1007,  2003,\n",
      "        1037, 11888, 20187,  1998, 11311,  1011, 19872,  4295,  2008,\n",
      "       13531,  1996,  3096,  1010, 10063,  1010,  2606,  1010,  1998,\n",
      "       14163, 27199, 24972,  1012,  1031,  1015,  1033,  2009,  2003,\n",
      "        7356,  2011, 26572, 20028,  1010,  4257,  1011,  9370,  1010,\n",
      "       10482,  3401,  3560,  6643, 14289,  4244,  1998, 28487,  2007,\n",
      "       15241,  2075,  1010,  2128,  4588,  8898,  1010,  2986,  2317,\n",
      "        4094,  1006, 15536,  3600,  3511,  1005,  1055,  2358,  4360,\n",
      "        2063,  1007,  1010,  4141, 12473, 12759,  2398,  1010, 23951,\n",
      "       11137, 12150,  1998, 27323,  1010,  8260,  1010, 15099,  2896,\n",
      "        3456,  1998,  8700, 14163, 13186,  2050,  1012,  1031,  1016,\n",
      "        1033,  2348,  2045,  2003,  1037,  5041,  6612,  2846,  1997,\n",
      "        6948, 24491,  2015,  1010,  1996,  3096,  1998,  8700, 17790,\n",
      "        3961,  2004,  1996,  2350,  4573,  1997,  6624,  1012,  1031,\n",
      "        1017,  1033,  1996,  3426,  2003,  4242,  1010,  2021,  2009,\n",
      "        2003,  2245,  2000,  2022,  1996,  2765,  1997,  2019,  8285,\n",
      "        5714, 23041,  2063,  2832,  2007,  2019,  4242,  3988,  9495,\n",
      "        1012,  2045,  2003,  2053,  9526,  1010,  2021,  2116,  2367,\n",
      "       20992,  1998,  8853,  2031,  2042,  2109,  1999,  4073,  2000,\n",
      "        2491,  1996,  8030,  1012,  1996,  2744,  5622,  8661,  9314,\n",
      "        4668,  1006,  5622,  8661,  9314, 17259,  2030,  5622,  8661,\n",
      "        9314,  4649,  3258,  1007,  5218,  2000,  1037,  4649,  3258,\n",
      "        1997,  2714,  2030,  7235,  2010, 14399,  8988, 12898, 12863,\n",
      "        1998,  6612,  3311,  2000,  5622,  8661,  2933,  2271,  1006,\n",
      "        1045,  1012,  1041,  1012,  1010,  2019,  2181,  2029, 12950,\n",
      "        5622,  8661,  2933,  2271,  1010,  2119,  2000,  1996,  6248,\n",
      "        3239,  1998,  2104,  1037, 24635,  1007,  1012,  1031,  1018,\n",
      "        1033,  1031,  1019,  1033,  2823, 11394,  4475,  2030,  3056,\n",
      "       20992,  2064,  3426,  1037,  5622,  8661,  9314,  4668,  1012,\n",
      "        1031,  1018,  1033,  2027,  2064,  2036,  5258,  1999,  2523,\n",
      "        2007, 22160,  2102,  6431,  3677,  4295,  1012,  1031,  1018,\n",
      "        1033,  1031,  1020,  1033,  1024, 24398,  8417,  1015,  5579,\n",
      "        1015,  1012,  1015,  2609,  1015,  1012,  1016,  5418,  1015,\n",
      "        1012,  1017, 17702,  8715,  2015,  1016,  5751,  1998,  8030,\n",
      "        1016,  1012,  1015,  3096,  1016,  1012,  1016, 14163, 27199,\n",
      "       24972,  1017,  5320,  1018, 26835, 19009,  1019, 11616,  1019,\n",
      "        1012,  1015,  3096,  1019,  1012,  1016,  2677,  1019,  1012,\n",
      "        1017, 11658, 11616,  1019,  1012,  1018,  2010, 14399,  8988,\n",
      "        6779,  1020,  3949,  1020,  1012,  1015,  3096,  1020,  1012,\n",
      "        1016,  2677,  1021,  4013, 26745,  6190,  1022,  4958,  5178,\n",
      "        4328,  6779,  1023,  2381,  2184,  2470,  2340,  3964,  2260,\n",
      "        7604,  2410,  6327,  6971,  5579,  5622,  8661,  2933,  2271,\n",
      "       22520,  2024,  2061,  2170,  2138,  1997,  2037,  1000,  5622,\n",
      "        8661,  1011,  2066,  1000,  3311,  1031,  1021,  1033,  1998,\n",
      "        2064,  2022,  6219,  2011,  1996,  2609,  2027,  9125,  1010,\n",
      "        2030,  2011,  2037, 19476,  1012,  2609,  5622,  8661,  2933,\n",
      "        2271,  2089,  2022, 20427,  2004, 12473, 14163, 13186,  2389,\n",
      "        2030,  3013, 17191,  9972,  1012,  3013, 17191,  3596,  2024,\n",
      "        2216, 12473,  1996,  3096,  1010, 21065,  1010,  1998, 10063,\n",
      "        1012,  1031,  1022,  1033,  1031,  1023,  1033,  1031,  2184,\n",
      "        1033, 14163, 13186,  2389,  3596,  2024,  2216, 12473,  1996,\n",
      "       14834,  1997,  1996,  3806, 13181, 18447, 19126, 12859,  1006,\n",
      "        2677,  1010,  6887,  5649, 26807,  1010,  9686,  7361,  3270,\n",
      "       12349,  1010,  4308,  1010,  2019,  2271,  1007,  1010,  2474,\n",
      "       18143,  2595,  1010,  1998,  2060, 14163, 13186,   102]), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1])}, 0)\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset.as_numpy_iterator():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification at 0x1b230170288>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (512,), attention_mask: (512,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1567 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4783 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4175 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4090 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (512, 2)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e3af099ff5ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\kajetan\\desktop\\dermclass\\src\\dermclass_models2\\dermclass_models2\\pipeline.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, model, train_dataset, validation_dataset, n_epochs)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1567 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4783 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4175 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:4090 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (512, 2)).\n"
     ]
    }
   ],
   "source": [
    "model2 = tm._train_model(model, train_dataset, validation_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dermclass_models2.pipeline import ImagePipeline \n",
    "from dermclass_models2.preprocessing import ImagePreprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = ImagePreprocessors()\n",
    "im = ImagePipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 14:09:02,271 — dermclass_models2.preprocessing — INFO —_get_avg_img_size:175 — Mean height is: 473, mean width is: 561\n",
      "2020-12-06 14:09:02,280 — dermclass_models2.preprocessing — INFO —_get_efficientnet_and_size:197 — Chosen model is <function EfficientNetB6 at 0x000001EE5A3FCDC8> with img_size (528, 528)\n",
      "Found 71 files belonging to 3 classes.\n",
      "Using 57 files for training.\n",
      "Found 71 files belonging to 3 classes.\n",
      "Using 14 files for validation.\n",
      "2020-12-06 14:09:03,949 — dermclass_models2.preprocessing — INFO —_load_dataset:225 — Successfully loaded train and validation datasets \n",
      "2020-12-06 14:09:03,953 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:120 — Successfully prefetched train, test and validation datasets\n",
      "2020-12-06 14:09:03,953 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:123 — Number of train batches: 19        Number of validation batches: 3        Number of test batches: 2\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = ip.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.fit_datasets(train_dataset, validation_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 14:09:03,973 — dermclass_models2.pipeline — INFO —set_img_size_and_model_obj:306 — Successfully set img size and model obj\n"
     ]
    }
   ],
   "source": [
    "im.set_img_size_and_model_obj(ip.img_size, ip.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 14:09:04,006 — dermclass_models2.pipeline — INFO —get_processing_pipeline:321 — Successfully loaded image processing pipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1ee3a87a3c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.get_processing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 14:09:10,647 — dermclass_models2.pipeline — WARNING —get_model:331 — Warning! get_model function in ImagePipeline returns unfitted model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1ef0dceae88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 14:09:10,679 — dermclass_models2.pipeline — INFO —get_processing_pipeline:321 — Successfully loaded image processing pipeline\n",
      "2020-12-06 14:09:17,174 — dermclass_models2.pipeline — WARNING —get_model:331 — Warning! get_model function in ImagePipeline returns unfitted model\n",
      "2020-12-06 14:09:19,269 — dermclass_models2.pipeline — INFO —_compile_model:206 — Successfully compiled tf model\n",
      "Epoch 1/5\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.0546 - accuracy: 0.5439"
     ]
    }
   ],
   "source": [
    "im.get_modeling_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 11:33:27,132 — dermclass_models2.preprocessing — INFO —_load_structured_data:137 — Successfully loaded data from csv\n",
      "2020-12-06 11:33:27,134 — dermclass_models2.preprocessing — INFO —_split_target_structured:57 — Successfully splat the target\n",
      "2020-12-06 11:33:27,136 — dermclass_models2.preprocessing — INFO —_split_train_test_structured:80 — Successfully splat train and test data\n",
      "2020-12-06 11:33:27,137 — dermclass_models2.preprocessing — INFO —_load_data_structured:87 — Successfully loaded the data\n"
     ]
    }
   ],
   "source": [
    "from dermclass_models2.pipeline import StructuredPipeline\n",
    "from dermclass_models2.preprocessing import StructuredPreprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "sp = StructuredPreprocessor()\n",
    "sm = StructuredPipeline()\n",
    "x_train, x_test, y_train, y_test = sp.load_data()\n",
    "sm.fit_structured_data(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 11:33:27,142 — dermclass_models2.pipeline — INFO —get_processing_pipeline:254 — Successfully loaded structured pipeline\n"
     ]
    }
   ],
   "source": [
    "pipe = sm.get_processing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 11:33:27,155 — dermclass_models2.pipeline — INFO —get_processing_pipeline:254 — Successfully loaded structured pipeline\n",
      "2020-12-06 11:33:27,242 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:144 — Finding hyperparameters for XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-06 11:33:27,243]\u001b[0m A new study created in memory with name: XGBClassifier\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:28,376]\u001b[0m Trial 0 finished with value: 0.5308577039063048 and parameters: {'subsample': 0.4, 'colsample_bytree': 0.6, 'colsample_bylevel': 0.8, 'min_child_weight': 9, 'max_depth': 3, 'max_delta_step': 4.2, 'learning_rate': 0.013599316932687493, 'n_estimators': 130, 'gamma': 8.5}. Best is trial 0 with value: 0.5308577039063048.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:28,516]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'subsample': 0.1, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.8, 'min_child_weight': 17, 'max_depth': 3, 'max_delta_step': 6.7, 'learning_rate': 0.06738479414901437, 'n_estimators': 197, 'gamma': 23.6}. Best is trial 0 with value: 0.5308577039063048.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:28,963]\u001b[0m Trial 5 finished with value: 0.8698015288589662 and parameters: {'subsample': 1.0, 'colsample_bytree': 0.6, 'colsample_bylevel': 1.0, 'min_child_weight': 1, 'max_depth': 10, 'max_delta_step': 8.0, 'learning_rate': 0.01822801937321195, 'n_estimators': 158, 'gamma': 23.000000000000004}. Best is trial 5 with value: 0.8698015288589662.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:29,060]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.6, 'min_child_weight': 18, 'max_depth': 3, 'max_delta_step': 6.6, 'learning_rate': 0.017463466579465684, 'n_estimators': 315, 'gamma': 18.500000000000004}. Best is trial 5 with value: 0.8698015288589662.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:29,444]\u001b[0m Trial 9 finished with value: 0.47945157444421066 and parameters: {'subsample': 0.30000000000000004, 'colsample_bytree': 1.0, 'colsample_bylevel': 0.7, 'min_child_weight': 1, 'max_depth': 8, 'max_delta_step': 6.7, 'learning_rate': 0.03951531923311127, 'n_estimators': 103, 'gamma': 23.800000000000004}. Best is trial 5 with value: 0.8698015288589662.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:29,569]\u001b[0m Trial 1 finished with value: 0.47945157444421066 and parameters: {'subsample': 0.4, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.6, 'min_child_weight': 11, 'max_depth': 8, 'max_delta_step': 3.1, 'learning_rate': 0.013713677712178426, 'n_estimators': 406, 'gamma': 14.700000000000001}. Best is trial 5 with value: 0.8698015288589662.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:29,681]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.7, 'colsample_bylevel': 1.0, 'min_child_weight': 9, 'max_depth': 11, 'max_delta_step': 9.4, 'learning_rate': 0.015649423407642787, 'n_estimators': 220, 'gamma': 3.5000000000000004}. Best is trial 5 with value: 0.8698015288589662.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:29,819]\u001b[0m Trial 4 finished with value: 0.9384599200504944 and parameters: {'subsample': 0.6, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.9, 'min_child_weight': 7, 'max_depth': 3, 'max_delta_step': 3.3000000000000003, 'learning_rate': 0.048108791090935236, 'n_estimators': 362, 'gamma': 2.4000000000000004}. Best is trial 4 with value: 0.9384599200504944.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:29,925]\u001b[0m Trial 6 finished with value: 0.8732379549758047 and parameters: {'subsample': 0.5, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9, 'min_child_weight': 3, 'max_depth': 9, 'max_delta_step': 4.7, 'learning_rate': 0.024853195105347562, 'n_estimators': 267, 'gamma': 11.4}. Best is trial 4 with value: 0.9384599200504944.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:30,656]\u001b[0m Trial 3 finished with value: 0.9589732800336629 and parameters: {'subsample': 0.5, 'colsample_bytree': 0.9, 'colsample_bylevel': 1.0, 'min_child_weight': 3, 'max_depth': 10, 'max_delta_step': 4.5, 'learning_rate': 0.025076337470097482, 'n_estimators': 428, 'gamma': 3.6}. Best is trial 3 with value: 0.9589732800336629.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 11:33:30,836 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:161 — Best params found for: XGBClassifier with score: 0.9589732800336629\n",
      "2020-12-06 11:33:31,421 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:166 — Successfully tuned hyperparameters\n",
      "2020-12-06 11:33:31,422 — dermclass_models2.pipeline — INFO —get_model:264 — Successfully loaded structured model\n"
     ]
    }
   ],
   "source": [
    "model = sm.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"Processing pipeline\", pipe), (\"Model\", model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Processing pipeline',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Cast dtypes',\n",
       "                                                  CastTypesTransformer(categorical_variables=[],\n",
       "                                                                       numeric_variables=['age'],\n",
       "                                                                       ordinal_variables=['erythema',\n",
       "                                                                                          'scaling',\n",
       "                                                                                          'definite_borders',\n",
       "                                                                                          'itching',\n",
       "                                                                                          'koebner_phenomenon',\n",
       "                                                                                          'polygonal_papules',\n",
       "                                                                                          'follicular_papules',\n",
       "                                                                                          'oral_mucosal_involvement',\n",
       "                                                                                          'knee_and_el...\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.025076337470097482,\n",
       "                               max_delta_step=4.5, max_depth=10,\n",
       "                               min_child_weight=3, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=428,\n",
       "                               n_jobs=0, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "                               subsample=0.5, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(sm.x_train, sm.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 11:33:33,274 — dermclass_models2.pipeline — INFO —get_processing_pipeline:254 — Successfully loaded structured pipeline\n",
      "2020-12-06 11:33:33,275 — dermclass_models2.pipeline — INFO —get_processing_pipeline:254 — Successfully loaded structured pipeline\n",
      "2020-12-06 11:33:33,352 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:144 — Finding hyperparameters for XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-06 11:33:33,353]\u001b[0m A new study created in memory with name: XGBClassifier\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:33,848]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 1.0, 'colsample_bylevel': 1.0, 'min_child_weight': 19, 'max_depth': 11, 'max_delta_step': 9.1, 'learning_rate': 0.023522161826835117, 'n_estimators': 100, 'gamma': 25.700000000000003}. Best is trial 1 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:34,261]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'subsample': 0.30000000000000004, 'colsample_bytree': 0.8, 'colsample_bylevel': 1.0, 'min_child_weight': 19, 'max_depth': 9, 'max_delta_step': 2.9000000000000004, 'learning_rate': 0.025745530988627207, 'n_estimators': 174, 'gamma': 18.700000000000003}. Best is trial 1 with value: 0.0.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:35,017]\u001b[0m Trial 9 finished with value: 0.4828529349884283 and parameters: {'subsample': 0.9, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.8, 'min_child_weight': 17, 'max_depth': 6, 'max_delta_step': 2.6, 'learning_rate': 0.010790746454989883, 'n_estimators': 107, 'gamma': 19.300000000000004}. Best is trial 9 with value: 0.4828529349884283.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:35,512]\u001b[0m Trial 7 finished with value: 0.0 and parameters: {'subsample': 0.30000000000000004, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.6, 'min_child_weight': 14, 'max_depth': 9, 'max_delta_step': 4.9, 'learning_rate': 0.03328546170521679, 'n_estimators': 448, 'gamma': 25.200000000000003}. Best is trial 9 with value: 0.4828529349884283.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:35,639]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'subsample': 0.30000000000000004, 'colsample_bytree': 0.8, 'colsample_bylevel': 1.0, 'min_child_weight': 14, 'max_depth': 11, 'max_delta_step': 1.6, 'learning_rate': 0.012948520677108583, 'n_estimators': 452, 'gamma': 25.700000000000003}. Best is trial 9 with value: 0.4828529349884283.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:35,775]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.8, 'min_child_weight': 5, 'max_depth': 4, 'max_delta_step': 4.9, 'learning_rate': 0.09104620784482612, 'n_estimators': 444, 'gamma': 17.3}. Best is trial 9 with value: 0.4828529349884283.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:35,844]\u001b[0m Trial 6 finished with value: 0.9316221333894382 and parameters: {'subsample': 0.9, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.8, 'min_child_weight': 4, 'max_depth': 7, 'max_delta_step': 5.1, 'learning_rate': 0.03678874427663821, 'n_estimators': 296, 'gamma': 15.700000000000001}. Best is trial 6 with value: 0.9316221333894382.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:36,125]\u001b[0m Trial 3 finished with value: 0.8904200855599972 and parameters: {'subsample': 0.6, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.8, 'min_child_weight': 5, 'max_depth': 5, 'max_delta_step': 4.3999999999999995, 'learning_rate': 0.05427313743004508, 'n_estimators': 376, 'gamma': 10.4}. Best is trial 6 with value: 0.9316221333894382.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:36,232]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.8, 'min_child_weight': 15, 'max_depth': 8, 'max_delta_step': 0.6, 'learning_rate': 0.029518501430788106, 'n_estimators': 491, 'gamma': 2.9000000000000004}. Best is trial 6 with value: 0.9316221333894382.\u001b[0m\n",
      "\u001b[32m[I 2020-12-06 11:33:36,768]\u001b[0m Trial 5 finished with value: 0.8148888421347921 and parameters: {'subsample': 0.7000000000000001, 'colsample_bytree': 1.0, 'colsample_bylevel': 0.9, 'min_child_weight': 4, 'max_depth': 4, 'max_delta_step': 7.3, 'learning_rate': 0.08357989725913789, 'n_estimators': 407, 'gamma': 19.500000000000004}. Best is trial 6 with value: 0.9316221333894382.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 11:33:36,936 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:161 — Best params found for: XGBClassifier with score: 0.9316221333894382\n",
      "2020-12-06 11:33:37,271 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:166 — Successfully tuned hyperparameters\n",
      "2020-12-06 11:33:37,272 — dermclass_models2.pipeline — INFO —get_model:264 — Successfully loaded structured model\n",
      "2020-12-06 11:33:38,194 — dermclass_models2.pipeline — INFO —get_modeling_pipeline:279 — Successfully loaded structured modeling pipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Processing pipeline',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Cast dtypes',\n",
       "                                                  CastTypesTransformer(categorical_variables=[],\n",
       "                                                                       numeric_variables=['age'],\n",
       "                                                                       ordinal_variables=['erythema',\n",
       "                                                                                          'scaling',\n",
       "                                                                                          'definite_borders',\n",
       "                                                                                          'itching',\n",
       "                                                                                          'koebner_phenomenon',\n",
       "                                                                                          'polygonal_papules',\n",
       "                                                                                          'follicular_papules',\n",
       "                                                                                          'oral_mucosal_involvement',\n",
       "                                                                                          'knee_and_el...\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.03678874427663821,\n",
       "                               max_delta_step=5.1, max_depth=7,\n",
       "                               min_child_weight=4, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=296,\n",
       "                               n_jobs=0, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "                               subsample=0.9, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.get_modeling_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dermclass_models2.preprocessing import StructuredPreprocessor, TextPreprocessors, ImagePreprocessors\n",
    "sp = StructuredPreprocessor()\n",
    "tp = TextPreprocessors()\n",
    "ip = ImagePreprocessors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 files belonging to 2 classes.\n",
      "Using 11 files for training.\n",
      "Found 13 files belonging to 2 classes.\n",
      "Using 11 files for training.\n",
      "2020-12-05 17:02:59,997 — dermclass_models2.preprocessing — INFO —_load_data_tf:130 — Successfully loaded train and validation datasets \n",
      "2020-12-05 17:03:00,000 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:152 — Number of train batches: 6        Number of validation batches: 3        Number of test batches: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>,\n",
       " <PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>,\n",
       " <PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.load_data(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.get_modeling_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dermclass_models2.preprocessing import TextPreprocessors\n",
    "from dermclass_models2.config import TextConfig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 13:41:14,036 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:264 — Successfully loaded class lichen_planus\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "pp = TextPreprocessors(TextConfig)\n",
    "df = pp._load_class_from_dir(TextConfig.DATA_PATH / \"lichen_planus\")\n",
    "\n",
    "df['encoded_cat'] = df['target'].astype('category').cat.codes\n",
    "\n",
    "data_texts = df[\"text\"].to_list() # Features (not-tokenized yet)\n",
    "data_labels = df[\"encoded_cat\"].to_list() # Lables\n",
    "\n",
    "# Split Train and Validation data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Keep some data for inference (testing)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 files belonging to 3 classes.\n",
      "Using 18 files for training.\n",
      "Found 22 files belonging to 3 classes.\n",
      "Using 4 files for validation.\n",
      "2020-12-06 13:41:15,624 — dermclass_models2.preprocessing — INFO —_load_dataset:301 — Successfully loaded train and validation datasets \n"
     ]
    }
   ],
   "source": [
    "raw_train_ds, raw_validation_ds = pp._load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertModel\n",
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_38', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1328\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 929us/step - loss: 0.9578\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c58e0cd348>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\n",
    "model.fit(train_dataset.batch(16), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (512,), attention_mask: (512,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_dataset(dataset, tokenizer):\n",
    "    text_to_encode = []\n",
    "    labels = []\n",
    "    for batch in dataset.as_numpy_iterator():\n",
    "        text_batch = batch[0]\n",
    "        labels_batch = batch[1]\n",
    "\n",
    "        text_batch_decoded = [text.decode(\"utf-8\") for text in text_batch]\n",
    "        text_to_encode += text_batch_decoded\n",
    "        labels += labels_batch.tolist()\n",
    "    text_encodings = tokenizer(text_to_encode, truncation=True, padding=True)\n",
    "    print(len(text_encodings[\"input_ids\"]))\n",
    "    print(len(labels))\n",
    "    print(labels)\n",
    "    text_batch_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(text_encodings),\n",
    "        labels))\n",
    "    return text_batch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_pipeline.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n",
      "[0, 1, 1, 1, 2, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 2, 2]\n",
      "({'input_ids': array([  101, 16770,  1024,  1013,  1013,  7479,  1012,  2740,  4179,\n",
      "        1012,  4012,  1013,  2740,  1013,  5622,  8661,  1011,  2933,\n",
      "        2271,  2054,  2003,  5622,  8661,  2933,  2271,  1029,  5622,\n",
      "        8661,  2933,  2271,  2003,  1037,  3096, 23438, 13330,  2011,\n",
      "        1996, 11311,  2291,  1012,  2009,  1521,  1055,  2025,  2124,\n",
      "        2339,  1996, 11311,  3433,  5158,  1012,  2045,  2089,  2022,\n",
      "        2195,  8020,  5876,  1010,  1998,  2169,  2553,  2003,  2367,\n",
      "        1012,  4022,  5320,  2421,  1024, 13434, 15245,  2035,  2121,\n",
      "       21230,  6911, 14471,  2823,  5622,  8661,  2933,  2271,  5158,\n",
      "        2247,  2007,  8285,  5714, 23041,  2063, 10840,  1012,  2096,\n",
      "        2009,  2089,  2022,  8796,  1010,  1999,  2087,  3572,  5622,\n",
      "        8661,  2933,  2271,  2003,  2025,  1037,  3809,  4650,  1012,\n",
      "        2009,  1521,  1055,  2036,  2025,  9530, 15900,  6313,  1012,\n",
      "        2174,  1010,  2045,  2024,  2070,  4678,  8358,  1997,  1996,\n",
      "        4650,  2008,  2089,  2022,  3809,  1998,  9145,  1012,  2122,\n",
      "        3785,  2064,  2022,  5845,  2007, 25665,  1998,  8700, 20992,\n",
      "        2000,  5547,  8030,  1010,  2030,  2011,  2478,  5850,  2008,\n",
      "       16081,  1996, 11311,  2291,  1012,  4620,  1997,  5622,  8661,\n",
      "        2933,  2271,  8030,  1997,  5622,  8661,  2933,  2271,  2070,\n",
      "        1997,  1996,  2087,  2691,  8030,  1997,  5622,  8661,  2933,\n",
      "        2271,  2421,  1996,  2206,  1024, 16405, 14536, 13602,  1011,\n",
      "        6910, 22520,  2030, 18548,  2007,  4257, 13284,  2006,  2115,\n",
      "        3096,  2030,  8991, 18400,  2015, 22520,  2008,  4503,  1998,\n",
      "        3659,  2058,  1996,  2303,  2058,  1996,  2607,  1997,  2195,\n",
      "        3134,  2030,  1037,  2261,  2706,  2009,  8450,  2012,  1996,\n",
      "        2609,  1997,  1996, 23438, 19959,  1011,  2317, 22520,  1999,\n",
      "        1996,  2677,  1010,  2029,  2089,  2022,  9145,  2030,  3426,\n",
      "        1037,  5255,  8742,  1038,  9863,  2545,  1010,  2029,  6532,\n",
      "        1998,  2468,  8040,  7875,  3762,  4857,  2317,  3210,  2058,\n",
      "        1996, 23438,  1996,  2087,  2691,  2828,  1997,  5622,  8661,\n",
      "        2933,  2271, 13531,  1996,  3096,  1012,  2058,  1996,  2607,\n",
      "        1997,  2195,  3134,  1010, 22520,  3711,  1998,  3659,  1012,\n",
      "        1996,  4650,  2788, 28837,  2039,  2306,  1020,  2000,  2385,\n",
      "        2706,  1012,  2625,  4141,  1010,  1996, 22520,  2064,  5258,\n",
      "        1999,  2752,  4661,  1996,  3096,  2030,  8991, 18400,  2015,\n",
      "        1012,  2122,  2089,  2421,  1024, 14163, 27199, 24972, 10063,\n",
      "        1996, 21065,  2045,  2024,  2036,  8358,  1997,  1996,  4650,\n",
      "        2062,  2691,  1999,  1996,  2690,  2264,  1010,  4021,  1010,\n",
      "        3088,  1010,  1998,  3763,  2637,  1012,  2054,  2024,  1996,\n",
      "        5320,  1998,  3891,  5876,  1029,  5622,  8661,  2933,  2271,\n",
      "       11791,  2043,  2115,  2303,  4491,  2115,  3096,  2030, 14163,\n",
      "       27199, 10804,  4442,  2011,  6707,  1012,  7435,  2024,  2025,\n",
      "        2469,  2339,  2023,  6433,  1012,  5622,  8661,  2933,  2271,\n",
      "        2064,  5258,  1999,  3087,  2012,  2151,  2287,  1010,  2021,\n",
      "        2045,  2024,  3056,  5876,  2008,  2191,  2070,  2111,  2062,\n",
      "        3497,  2000,  4503,  1996,  4650,  1012,  1996,  3096,  2433,\n",
      "        1997,  5622,  8661,  2933,  2271,  5158,  1999,  2273,  1998,\n",
      "        2308,  8053,  1010,  2021,  2308,  2024,  3807,  2004,  3497,\n",
      "        2000,  2131,  1996,  8700,  2433,  1012,  2009,  1521,  1055,\n",
      "        2200,  4678,  1999,  2336,  1998,  3080,  6001,  1012,  2009,\n",
      "        1521,  1055,  2087,  2691,  1999,  2690,  1011,  4793,  2111,\n",
      "        1012,  2060,  3891,  5876,  2421,  2383,  2155,  2372,  2040,\n",
      "        1521,  2310,  2018,  5622,  8661,  2933,  2271,  1010,  2383,\n",
      "        1037, 13434,  4295,  2066, 28389,  1039,  1010,  2030,  2108,\n",
      "        6086,  2000,  3056, 12141,  2008,  2552,  2004,  2035,  2121,\n",
      "       21230,  1012,  2122,  2035,  2121, 21230,  2089,  2421,  1024,\n",
      "       24479, 29596,  2751, 22834,  4305,  3207, 10099,   102]), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1])}, 0)\n"
     ]
    }
   ],
   "source": [
    "train_ds_encoded = _encode_dataset(raw_train_ds, tokenizer)\n",
    "for i in train_ds_encoded.as_numpy_iterator():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorSliceDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-3d31b640feae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    588\u001b[0m     )\n\u001b[0;32m    589\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistilbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mshape_list\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m     \"\"\"\n\u001b[1;32m-> 1050\u001b[1;33m     \u001b[0mstatic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m     \u001b[0mdynamic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TensorSliceDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'input_ids': <tf.Tensor 'ExpandDims_1:0' shape=(512, 1) dtype=int32>, 'attention_mask': <tf.Tensor 'ExpandDims:0' shape=(512, 1) dtype=int32>}\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:386 call\n        outputs = layer(inputs, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\pooling.py:788 call\n        return backend.mean(inputs, axis=steps_axis)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:2261 mean\n        if x.dtype.base_dtype == dtypes_module.bool:\n\n    AttributeError: 'TFBaseModelOutput' object has no attribute 'dtype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-9413f4982fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodeling_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:386 call\n        outputs = layer(inputs, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\pooling.py:788 call\n        return backend.mean(inputs, axis=steps_axis)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Kajetan\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:2261 mean\n        if x.dtype.base_dtype == dtypes_module.bool:\n\n    AttributeError: 'TFBaseModelOutput' object has no attribute 'dtype'\n"
     ]
    }
   ],
   "source": [
    "modeling_pipeline.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline prepare model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(df['target'].unique()))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataset:\n",
    "    batch = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       " array([b\"https://www.nhs.uk/conditions/lichen-planus/\\r\\n\\r\\nLichen planus\\r\\nLichen planus is a rash that can affect different parts of your body, including the inside of your mouth. See a GP if you think you might have it.\\r\\n\\r\\nNon-urgent advice:See a GP if you have:\\r\\nclusters of shiny, raised, purple-red blotches on your arms, legs or body (you may see fine white lines on the blotches)\\r\\nwhite patches on your gums, tongue or the insides of your cheeks\\r\\nburning and stinging in your mouth, especially when you eat or drink\\r\\nbald patches appearing on your scalp\\r\\nsore red patches on your vulva\\r\\nrough, thinning nails with grooves on\\r\\nring-shaped purple or white patches on your penis\\r\\nThese are symptoms of lichen planus. You may only have 1 of these symptoms.\\r\\n\\r\\nLichen planus on your skin can be very itchy, but not always.\\r\\n\\r\\nInformation:\\r\\nCoronavirus update: how to contact a GP\\r\\nIt's still important to get help from a GP if you need it. To contact your GP surgery:\\r\\n\\r\\nvisit their website\\r\\nuse the NHS App\\r\\ncall them\\r\\nFind out about using the NHS during coronavirus\\r\\n\\r\\nLichen planus on the inside of the wrist\\r\\nLichen planus often appears on the inside of your wrist\\r\\nWhite patches of lichen planus in the mouth\\r\\nWhite patches in your mouth may be lichen planus\\r\\nIf you're not sure it's lichen planus\\r\\nTreatments from a GP\\r\\nLichen planus on your skin usually gets better on its own in about 9 to 18 months.\\r\\n\\r\\nCreams and ointments from a GP can help control the rash and ease itching.\\r\\n\\r\\nIf creams and ointments do not work or you have severe lichen planus, steroid tablets or treatment with a special kind of light (light therapy) can help.\\r\\n\\r\\nLichen planus in your mouth can last for several years. Mouthwashes and sprays from a GP can help ease symptoms like burning or sore gums.\\r\\n\\r\\nYou cannot catch lichen planus and it does not usually come back once it's cleared up.\\r\\n\\r\\nFor support and information, see UK Lichen Planus.\\r\\n\\r\\nHow to relieve lichen planus at home\\r\\nIf you have lichen planus on your skin:\\r\\nwash with plain warm water \\xe2\\x80\\x93 avoid soaps and body washes\\r\\nwash your hair over a sink or bath so the shampoo does not come into contact with the rest of your skin\\r\\nuse an emollient (moisturising treatment for the skin) on the rash\\r\\nIf the lichen planus is on your genitals:\\r\\nhold a bag of frozen peas wrapped in a clean tea towel against the affected bits to ease itching and swelling\\r\\navoid wearing tights or close-fitting clothes\\r\\nIf you have it in your mouth:\\r\\nbrush your teeth carefully twice a day to keep your gums healthy\\r\\navoid salty, spicy or acidic foods if they make your mouth sore\\r\\navoid alcohol and mouthwashes that contain it\",\n",
       "        b\"https://www.nhs.uk/conditions/psoriasis/symptoms/\\r\\n\\r\\nMain symptoms of psoriasis\\r\\nPsoriasis typically causes patches of skin that are dry, red and covered in silver scales. Some people find their psoriasis causes itching or soreness.\\r\\n\\r\\nThere are several different types of psoriasis. Many people have only 1 form at a time, although 2 different types can occur together. One form may change into another or become more severe.\\r\\n\\r\\nMost cases of psoriasis go through cycles, causing problems for a few weeks or months before easing or stopping.\\r\\n\\r\\nYou should see a GP if you think you may have psoriasis.\\r\\n\\r\\nCommon types of psoriasis\\r\\nPlaque psoriasis (psoriasis vulgaris)\\r\\nPlaque psoriasis rash.\\r\\nThis is the most common form, accounting for about 80 to 90% of cases.\\r\\n\\r\\nIts symptoms are dry red skin lesions, known as plaques, covered in silver scales.\\r\\n\\r\\nThey normally appear on your elbows, knees, scalp and lower back, but can appear anywhere on your body.\\r\\n\\r\\nThe plaques can be itchy or sore, or both. In severe cases, the skin around your joints may crack and bleed. \\r\\n\\r\\nScalp psoriasis\\r\\nThis can occur on parts of your scalp or on the whole scalp. It causes red patches of skin covered in thick, silvery-white scales.\\r\\n\\r\\nSome people find scalp psoriasis extremely itchy, while others have no discomfort.\\r\\n\\r\\nIn extreme cases, it can cause hair loss, although this is usually only temporary.\\r\\n\\r\\nNail psoriasis\\r\\nIn about half of all people with psoriasis, the condition affects the nails.\\r\\n\\r\\nPsoriasis can cause your nails to develop tiny dents or pits, become discoloured or grow abnormally.\\r\\n\\r\\nNails can often become loose and separate from the nail bed. In severe cases, nails may crumble.\\r\\n\\r\\nGuttate psoriasis\\r\\nGuttate psoriasis causes small (less than 1cm) drop-shaped sores on your chest, arms, legs and scalp.\\r\\n\\r\\nThere's a good chance guttate psoriasis will disappear completely after a few weeks, but some people go on to develop plaque psoriasis.\\r\\n\\r\\nThis type of psoriasis sometimes occurs after a streptococcal throat infection and is more common among children and teenagers.\\r\\n\\r\\nInverse (flexural) psoriasis\\r\\nThis affects folds or creases in your skin, such as the armpits, groin, between the buttocks and under the breasts.\\r\\n\\r\\nIt can cause large, smooth red patches in some or all these areas.\\r\\n\\r\\nInverse psoriasis is made worse by friction and sweating, so it can be particularly uncomfortable in hot weather.\\r\\n\\r\\nLess common types of psoriasis\\r\\nPustular psoriasis\\r\\nPustular psoriasis is a rarer type of psoriasis that causes pus-filled blisters (pustules) to appear on your skin.\\r\\n\\r\\nDifferent types of pustular psoriasis affect different parts of the body.\\r\\n\\r\\nGeneralised pustular psoriasis or von Zumbusch psoriasis\\r\\nThis causes pustules that develop very quickly on a wide area of skin. The pus consists of white blood cells and is not a sign of infection.\\r\\n\\r\\nThe pustules may reappear every few days or weeks in cycles. During the start of these cycles, von Zumbusch psoriasis can cause fever, chills, weight loss and fatigue.\\r\\n\\r\\nPalmoplantar pustulosis\\r\\nThis causes pustules to appear on the palms of your hands and the soles of your feet.\\r\\n\\r\\nThe pustules gradually develop into circular, brown, scaly spots that then peel off.\\r\\n\\r\\nPustules may reappear every few days or weeks.\\r\\n\\r\\nAcropustulosis\\r\\nThis causes pustules to appear on your fingers and toes.\\r\\n\\r\\nThe pustules then burst, leaving bright red areas that may ooze or become scaly. These may lead to painful nail deformities.\\r\\n\\r\\nErythrodermic psoriasis\\r\\nErythrodermic psoriasis is a rare form of psoriasis that affects nearly all the skin on the body. This can cause intense itching or burning.\\r\\n\\r\\nErythrodermic psoriasis can cause your body to lose proteins and fluid, leading to further problems such as infection, dehydration, heart failure, hypothermia and malnutrition.\"],\n",
       "       dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1])>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       "  array([  101, 16770,  1024,  1013,  1013,  4372,  1012, 16948,  1012,\n",
       "          8917,  1013, 15536,  3211,  1013,  5622,  8661,  1035,  2933,\n",
       "          2271,  5622,  8661,  2933,  2271,  1006,  6948,  1007,  2003,\n",
       "          1037, 11888, 20187,  1998, 11311,  1011, 19872,  4295,  2008,\n",
       "         13531,  1996,  3096,  1010, 10063,  1010,  2606,  1010,  1998,\n",
       "         14163, 27199, 24972,  1012,  1031,  1015,  1033,  2009,  2003,\n",
       "          7356,  2011, 26572, 20028,  1010,  4257,  1011,  9370,  1010,\n",
       "         10482,  3401,  3560,  6643, 14289,  4244,  1998, 28487,  2007,\n",
       "         15241,  2075,  1010,  2128,  4588,  8898,  1010,  2986,  2317,\n",
       "          4094,  1006, 15536,  3600,  3511,  1005,  1055,  2358,  4360,\n",
       "          2063,  1007,  1010,  4141, 12473, 12759,  2398,  1010, 23951,\n",
       "         11137, 12150,  1998, 27323,  1010,  8260,  1010, 15099,  2896,\n",
       "          3456,  1998,  8700, 14163, 13186,  2050,  1012,  1031,  1016,\n",
       "          1033,  2348,  2045,  2003,  1037,  5041,  6612,  2846,  1997,\n",
       "          6948, 24491,  2015,  1010,  1996,  3096,  1998,  8700, 17790,\n",
       "          3961,  2004,  1996,  2350,  4573,  1997,  6624,  1012,  1031,\n",
       "          1017,  1033,  1996,  3426,  2003,  4242,  1010,  2021,  2009,\n",
       "          2003,  2245,  2000,  2022,  1996,  2765,  1997,  2019,  8285,\n",
       "          5714, 23041,  2063,  2832,  2007,  2019,  4242,  3988,  9495,\n",
       "          1012,  2045,  2003,  2053,  9526,  1010,  2021,  2116,  2367,\n",
       "         20992,  1998,  8853,  2031,  2042,  2109,  1999,  4073,  2000,\n",
       "          2491,  1996,  8030,  1012,  1996,  2744,  5622,  8661,  9314,\n",
       "          4668,  1006,  5622,  8661,  9314, 17259,  2030,  5622,  8661,\n",
       "          9314,  4649,  3258,  1007,  5218,  2000,  1037,  4649,  3258,\n",
       "          1997,  2714,  2030,  7235,  2010, 14399,  8988, 12898, 12863,\n",
       "          1998,  6612,  3311,  2000,  5622,  8661,  2933,  2271,  1006,\n",
       "          1045,  1012,  1041,  1012,  1010,  2019,  2181,  2029, 12950,\n",
       "          5622,  8661,  2933,  2271,  1010,  2119,  2000,  1996,  6248,\n",
       "          3239,  1998,  2104,  1037, 24635,  1007,  1012,  1031,  1018,\n",
       "          1033,  1031,  1019,  1033,  2823, 11394,  4475,  2030,  3056,\n",
       "         20992,  2064,  3426,  1037,  5622,  8661,  9314,  4668,  1012,\n",
       "          1031,  1018,  1033,  2027,  2064,  2036,  5258,  1999,  2523,\n",
       "          2007, 22160,  2102,  6431,  3677,  4295,  1012,  1031,  1018,\n",
       "          1033,  1031,  1020,  1033,  1024, 24398,  8417,  1015,  5579,\n",
       "          1015,  1012,  1015,  2609,  1015,  1012,  1016,  5418,  1015,\n",
       "          1012,  1017, 17702,  8715,  2015,  1016,  5751,  1998,  8030,\n",
       "          1016,  1012,  1015,  3096,  1016,  1012,  1016, 14163, 27199,\n",
       "         24972,  1017,  5320,  1018, 26835, 19009,  1019, 11616,  1019,\n",
       "          1012,  1015,  3096,  1019,  1012,  1016,  2677,  1019,  1012,\n",
       "          1017, 11658, 11616,  1019,  1012,  1018,  2010, 14399,  8988,\n",
       "          6779,  1020,  3949,  1020,  1012,  1015,  3096,  1020,  1012,\n",
       "          1016,  2677,  1021,  4013, 26745,  6190,  1022,  4958,  5178,\n",
       "          4328,  6779,  1023,  2381,  2184,  2470,  2340,  3964,  2260,\n",
       "          7604,  2410,  6327,  6971,  5579,  5622,  8661,  2933,  2271,\n",
       "         22520,  2024,  2061,  2170,  2138,  1997,  2037,  1000,  5622,\n",
       "          8661,  1011,  2066,  1000,  3311,  1031,  1021,  1033,  1998,\n",
       "          2064,  2022,  6219,  2011,  1996,  2609,  2027,  9125,  1010,\n",
       "          2030,  2011,  2037, 19476,  1012,  2609,  5622,  8661,  2933,\n",
       "          2271,  2089,  2022, 20427,  2004, 12473, 14163, 13186,  2389,\n",
       "          2030,  3013, 17191,  9972,  1012,  3013, 17191,  3596,  2024,\n",
       "          2216, 12473,  1996,  3096,  1010, 21065,  1010,  1998, 10063,\n",
       "          1012,  1031,  1022,  1033,  1031,  1023,  1033,  1031,  2184,\n",
       "          1033, 14163, 13186,  2389,  3596,  2024,  2216, 12473,  1996,\n",
       "         14834,  1997,  1996,  3806, 13181, 18447, 19126, 12859,  1006,\n",
       "          2677,  1010,  6887,  5649, 26807,  1010,  9686,  7361,  3270,\n",
       "         12349,  1010,  4308,  1010,  2019,  2271,  1007,  1010,  2474,\n",
       "         18143,  2595,  1010,  1998,  2060, 14163, 13186,   102])>,\n",
       "  'attention_mask': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       "  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1])>},\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 4s 2s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 5.1896e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x208b2d99108>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline training\n",
    "model.fit(train_dataset.shuffle(1000).batch(2), epochs=1, batch_size=16, validation_data=val_dataset.shuffle(1000).batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-9d01dfc21c99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2281\u001b[0m             )\n\u001b[0;32m   2282\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2283\u001b[1;33m             \u001b[1;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2284\u001b[0m             \u001b[1;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m         )\n",
      "\u001b[1;31mAssertionError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "pipe.tokenizer(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[-0.12484591],\n",
       "       [-0.05797866],\n",
       "       [-0.10238954],\n",
       "       ...,\n",
       "       [-0.1679904 ],\n",
       "       [-0.18195675],\n",
       "       [-0.09280173]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(15, 768), b.shape=(768, 768), m=15, n=768, k=768 [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d846a095e40f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfm_pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'distilbert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'distilbert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextClassificationPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\auto\\modeling_tf_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m             return TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING[type(config)].from_pretrained(\n\u001b[1;32m-> 1150\u001b[1;33m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m             )\n\u001b[0;32m   1152\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_pytorch_checkpoint_in_tf2_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_missing_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build the network with dummy inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Error retrieving file {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m         )\n\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m         )\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    363\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             \u001b[0mlayer_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, training)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \"\"\"\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# Self-Attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0msa_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0msa_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msa_output\u001b[0m  \u001b[1;31m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions, training)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_heads\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_lin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, n_heads, q_length, dim_per_head)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_lin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_lin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         dtype=self._compute_dtype_object)\n\u001b[0m\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\ops\\core.py\u001b[0m in \u001b[0;36mdense\u001b[1;34m(inputs, kernel, bias, activation, dtype)\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[1;31m# Broadcast kernel to inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandard_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# Reshape the output back to the original ndim of the input.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes, name)\u001b[0m\n\u001b[0;32m   4516\u001b[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001b[0;32m   4517\u001b[0m         b, b_axes, True)\n\u001b[1;32m-> 4518\u001b[1;33m     \u001b[0mab_matmul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_reshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4519\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_free_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_free_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4520\u001b[0m       if (ab_matmul.get_shape().is_fully_defined() and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   3252\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3253\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 3254\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   3255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5622\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5623\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5624\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5625\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5626\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(15, 768), b.shape=(768, 768), m=15, n=768, k=768 [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, pipeline as tfm_pipeline\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5203559398651123}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"asddd ssss \", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[  101,  2004, 14141,  2094,  7020,  4757,  4241,  4502,   102]])>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"asddd ssss \",\n",
    "                               truncation=True,\n",
    "                               padding=True,\n",
    "                               return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = tfm_pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, framework=\"tf\")\n",
    "score = pipeline.model.predict(pipeline.tokenizer.encode(\"asddd ssss \",\n",
    "                               truncation=True,\n",
    "                               padding=True,\n",
    "                               return_tensors=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.nn import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5203559398651123}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"asddd ssss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='./temp', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.02770592, -0.05376299]], dtype=float32),)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokenizer.encode(\"asddd ssss Dupa\",\n",
    "                               truncation=True,\n",
    "                               padding=True,\n",
    "                               return_tensors=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save_pretrained(\"./temp_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./saved_models were not used when initializing TFDistilBertForSequenceClassification: ['dropout_39']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./saved_models and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "save_directory = \"./saved_models\" # change this to your preferred location\n",
    "\n",
    "model.save_pretrained(\"./saved_models\")\n",
    "tokenizer.save_pretrained(\"./saved_models\")\n",
    "\n",
    "loaded_tokenizer = DistilBertTokenizer.from_pretrained(save_directory)\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='./saved_models', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = tf.keras.models.load_model(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-e0252d67b2f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./temp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \"\"\"\n\u001b[0;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1979\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    122\u001b[0m         not isinstance(model, sequential.Sequential)):\n\u001b[0;32m    123\u001b[0m       raise NotImplementedError(\n\u001b[1;32m--> 124\u001b[1;33m           \u001b[1;34m'Saving the model to HDF5 format requires the model to be a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m           \u001b[1;34m'Functional model or a Sequential model. It does not work for '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m           \u001b[1;34m'subclassed models, because such models are defined via the body of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model.save(\"./temp\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(tokenizer, \"./test.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-b4bea39108cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m predict_input = loaded_tokenizer.encode(test_text,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                  \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                  return_tensors=\"tf\")\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "predict_input = loaded_tokenizer.encode(test_text,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\")\n",
    "\n",
    "output = loaded_model(predict_input)[0]\n",
    "\n",
    "prediction_value = tf.argmax(output, axis=1).numpy()[0]\n",
    "prediction_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
