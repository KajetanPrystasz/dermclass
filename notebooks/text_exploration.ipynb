{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dermclass_models2.pipeline import TextModels \n",
    "from dermclass_models2.preprocessing import TextPreprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 files belonging to 2 classes.\n",
      "Using 11 files for training.\n",
      "Found 13 files belonging to 2 classes.\n",
      "Using 11 files for training.\n",
      "2020-12-04 22:37:58,651 — dermclass_models2.preprocessing — INFO —_load_data_tf:130 — Successfully loaded train and validation datasets \n",
      "2020-12-04 22:37:58,655 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:152 — Number of train batches: 6        Number of validation batches: 3        Number of test batches: 3\n",
      "2020-12-04 22:37:58,664 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:272 — Successfully loaded class lichen_planus\n",
      "2020-12-04 22:37:58,676 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:272 — Successfully loaded class psoriasis\n",
      "2020-12-04 22:37:58,678 — dermclass_models2.preprocessing — INFO —_load_data_from_files:285 — Successfully loaded the data\n",
      "2020-12-04 22:37:58,680 — dermclass_models2.preprocessing — INFO —_split_target_structured:55 — Successfully splat the target data\n",
      "2020-12-04 22:37:58,682 — dermclass_models2.preprocessing — INFO —_split_train_test_structured:78 — Successfully splat train and test data\n",
      "2020-12-04 22:37:58,683 — dermclass_models2.preprocessing — INFO —_load_data_structured:85 — Successfully loaded the data\n"
     ]
    }
   ],
   "source": [
    "tp = TextPreprocessors()\n",
    "tm = TextModels()\n",
    "train_dataset, validation_dataset, test_dataset = tp.load_data(get_datasets=True)\n",
    "x_train, x_test, y_train, y_test = tp.load_data(get_datasets=False)\n",
    "tm.fit_data(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TextModels._encode_dataset of <dermclass_models2.pipeline.TextModels object at 0x000001FD282FC548>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.get_processing_pipeline(use_sklearn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification at 0x1fd2e934188>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.get_model(use_sklearn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dermclass_models2.pipeline._TransformersModelingPipeline at 0x1fd36d94088>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.get_modeling_pipeline(use_sklearn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dermclass_models2.pipeline import ImageModels \n",
    "from dermclass_models2.preprocessing import ImagePreprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = ImagePreprocessors()\n",
    "im = ImageModels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-04 21:24:08,276 — dermclass_models2.preprocessing — INFO —_get_avg_img_size:205 — Mean height is: 473, mean width is: 561\n",
      "2020-12-04 21:24:08,277 — dermclass_models2.preprocessing — INFO —_get_efficientnet_and_size:229 — Chosen model is <function EfficientNetB6 at 0x0000021A71601DC8> with img_size (528, 528)\n",
      "Found 71 files belonging to 3 classes.\n",
      "Using 57 files for training.\n",
      "Found 71 files belonging to 3 classes.\n",
      "Using 57 files for training.\n",
      "2020-12-04 21:24:09,970 — dermclass_models2.preprocessing — INFO —_load_data_tf:130 — Successfully loaded train and validation datasets \n",
      "2020-12-04 21:24:09,974 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:152 — Number of train batches: 29        Number of validation batches: 15        Number of test batches: 14\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = ip.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.set_img_size_and_model_obj(ip.img_size, ip.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x21a135aab08>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.get_processing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x21b0dd8ce08>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x21b15f74f88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.get_modeling_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-04 21:42:02,270 — dermclass_models2.preprocessing — INFO —_load_data_from_files:166 — Data loaded from csv\n",
      "2020-12-04 21:42:02,272 — dermclass_models2.preprocessing — INFO —_split_target_structured:55 — Successfully splat the target data\n",
      "2020-12-04 21:42:02,273 — dermclass_models2.preprocessing — INFO —_split_train_test_structured:78 — Successfully splat train and test data\n",
      "2020-12-04 21:42:02,275 — dermclass_models2.preprocessing — INFO —_load_data_structured:85 — Successfully loaded the data\n"
     ]
    }
   ],
   "source": [
    "from dermclass_models2.pipeline import StructuredModels \n",
    "from dermclass_models2.preprocessing import StructuredPreprocessor\n",
    "sp = StructuredPreprocessor()\n",
    "sm = StructuredModels()\n",
    "x_train, x_test, y_train, y_test = sp.load_data()\n",
    "sm.fit_data(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(remainder='passthrough',\n",
       "                  transformers=[('Cast dtypes',\n",
       "                                 CastTypesTransformer(categorical_variables=[],\n",
       "                                                      numeric_variables=['age'],\n",
       "                                                      ordinal_variables=['erythema',\n",
       "                                                                         'scaling',\n",
       "                                                                         'definite_borders',\n",
       "                                                                         'itching',\n",
       "                                                                         'koebner_phenomenon',\n",
       "                                                                         'polygonal_papules',\n",
       "                                                                         'follicular_papules',\n",
       "                                                                         'oral_mucosal_involvement',\n",
       "                                                                         'knee_and_elbow_involvement',\n",
       "                                                                         'scalp_involvement',\n",
       "                                                                         '...\n",
       "                                  'exocytosis', 'acanthosis', 'hyperkeratosis',\n",
       "                                  'parakeratosis',\n",
       "                                  'clubbing_of_the_rete_ridges',\n",
       "                                  'elongation_of_the_rete_ridges',\n",
       "                                  'thinning_of_the_suprapapillary_epidermis',\n",
       "                                  'spongiform_pustule', 'munro_microabcess',\n",
       "                                  'focal_hypergranulosis',\n",
       "                                  'disappearance_of_the_granular_layer',\n",
       "                                  'vacuolisation_and_damage_of_basal_layer',\n",
       "                                  'spongiosis', 'saw_tooth_appearance_of_retes',\n",
       "                                  'follicular_horn_plug', ...])])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.get_processing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-04 21:42:04,472 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:131 — Finding hyperparameters for XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:04,473]\u001b[0m A new study created in memory with name: XGBClassifier\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:05,964]\u001b[0m Trial 7 finished with value: 0.4794520547945206 and parameters: {'subsample': 0.9, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.6, 'min_child_weight': 17, 'max_depth': 7, 'max_delta_step': 1.2000000000000002, 'learning_rate': 0.01869313536265822, 'n_estimators': 156, 'gamma': 23.800000000000004}. Best is trial 7 with value: 0.4794520547945206.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:06,102]\u001b[0m Trial 6 finished with value: 0.0 and parameters: {'subsample': 0.30000000000000004, 'colsample_bytree': 0.6, 'colsample_bylevel': 0.6, 'min_child_weight': 7, 'max_depth': 3, 'max_delta_step': 9.700000000000001, 'learning_rate': 0.03762786807266059, 'n_estimators': 196, 'gamma': 26.6}. Best is trial 7 with value: 0.4794520547945206.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:06,133]\u001b[0m Trial 5 finished with value: 0.2773972602739726 and parameters: {'subsample': 0.30000000000000004, 'colsample_bytree': 1.0, 'colsample_bylevel': 0.6, 'min_child_weight': 13, 'max_depth': 5, 'max_delta_step': 6.8, 'learning_rate': 0.013215681597032054, 'n_estimators': 210, 'gamma': 19.400000000000002}. Best is trial 7 with value: 0.4794520547945206.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:06,679]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'subsample': 0.5, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.6, 'min_child_weight': 20, 'max_depth': 4, 'max_delta_step': 5.7, 'learning_rate': 0.0783159846379465, 'n_estimators': 315, 'gamma': 28.700000000000003}. Best is trial 7 with value: 0.4794520547945206.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:07,132]\u001b[0m Trial 4 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.6, 'min_child_weight': 11, 'max_depth': 4, 'max_delta_step': 0.30000000000000004, 'learning_rate': 0.021727737007387938, 'n_estimators': 443, 'gamma': 27.500000000000004}. Best is trial 7 with value: 0.4794520547945206.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:07,136]\u001b[0m Trial 2 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9, 'min_child_weight': 2, 'max_depth': 12, 'max_delta_step': 5.3, 'learning_rate': 0.012801835298714419, 'n_estimators': 293, 'gamma': 11.200000000000001}. Best is trial 7 with value: 0.4794520547945206.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:07,154]\u001b[0m Trial 3 finished with value: 0.8013698630136986 and parameters: {'subsample': 0.9, 'colsample_bytree': 1.0, 'colsample_bylevel': 0.9, 'min_child_weight': 10, 'max_depth': 10, 'max_delta_step': 1.9000000000000001, 'learning_rate': 0.016313338214789926, 'n_estimators': 225, 'gamma': 19.400000000000002}. Best is trial 3 with value: 0.8013698630136986.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:07,669]\u001b[0m Trial 9 finished with value: 0.8904109589041096 and parameters: {'subsample': 0.9, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9, 'min_child_weight': 14, 'max_depth': 8, 'max_delta_step': 0.9, 'learning_rate': 0.09354059506196463, 'n_estimators': 163, 'gamma': 13.1}. Best is trial 9 with value: 0.8904109589041096.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:08,136]\u001b[0m Trial 14 finished with value: 0.0 and parameters: {'subsample': 0.5, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.7, 'min_child_weight': 16, 'max_depth': 7, 'max_delta_step': 5.0, 'learning_rate': 0.04407987409333039, 'n_estimators': 128, 'gamma': 21.300000000000004}. Best is trial 9 with value: 0.8904109589041096.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:08,211]\u001b[0m Trial 1 finished with value: 0.7945205479452054 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.9, 'colsample_bylevel': 1.0, 'min_child_weight': 2, 'max_depth': 8, 'max_delta_step': 2.9000000000000004, 'learning_rate': 0.0157384031000442, 'n_estimators': 426, 'gamma': 8.5}. Best is trial 9 with value: 0.8904109589041096.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:08,381]\u001b[0m Trial 8 finished with value: 0.0 and parameters: {'subsample': 0.5, 'colsample_bytree': 1.0, 'colsample_bylevel': 0.8, 'min_child_weight': 15, 'max_depth': 9, 'max_delta_step': 5.4, 'learning_rate': 0.01282652212655149, 'n_estimators': 271, 'gamma': 22.500000000000004}. Best is trial 9 with value: 0.8904109589041096.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:08,689]\u001b[0m Trial 10 finished with value: 0.0 and parameters: {'subsample': 0.1, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.7, 'min_child_weight': 7, 'max_depth': 9, 'max_delta_step': 9.6, 'learning_rate': 0.010241288375759118, 'n_estimators': 451, 'gamma': 6.8}. Best is trial 9 with value: 0.8904109589041096.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:09,312]\u001b[0m Trial 17 finished with value: 0.9623287671232876 and parameters: {'subsample': 1.0, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.8, 'min_child_weight': 6, 'max_depth': 10, 'max_delta_step': 3.2, 'learning_rate': 0.096054897621605, 'n_estimators': 109, 'gamma': 1.5000000000000002}. Best is trial 17 with value: 0.9623287671232876.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:09,439]\u001b[0m Trial 18 finished with value: 0.9554794520547945 and parameters: {'subsample': 1.0, 'colsample_bytree': 0.9, 'colsample_bylevel': 1.0, 'min_child_weight': 7, 'max_depth': 11, 'max_delta_step': 2.2, 'learning_rate': 0.09219680152727253, 'n_estimators': 113, 'gamma': 0.2}. Best is trial 17 with value: 0.9623287671232876.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:09,702]\u001b[0m Trial 11 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.7, 'min_child_weight': 10, 'max_depth': 7, 'max_delta_step': 3.2, 'learning_rate': 0.05119551601174705, 'n_estimators': 488, 'gamma': 1.4000000000000001}. Best is trial 17 with value: 0.9623287671232876.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:09,760]\u001b[0m Trial 16 finished with value: 0.9143835616438356 and parameters: {'subsample': 0.7000000000000001, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.7, 'min_child_weight': 7, 'max_depth': 6, 'max_delta_step': 4.5, 'learning_rate': 0.021377586032420716, 'n_estimators': 168, 'gamma': 8.6}. Best is trial 17 with value: 0.9623287671232876.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:09,777]\u001b[0m Trial 13 finished with value: 0.7945205479452054 and parameters: {'subsample': 0.7000000000000001, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.6, 'min_child_weight': 4, 'max_depth': 5, 'max_delta_step': 8.2, 'learning_rate': 0.010859320177706999, 'n_estimators': 260, 'gamma': 25.400000000000002}. Best is trial 17 with value: 0.9623287671232876.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:09,816]\u001b[0m Trial 19 finished with value: 0.8664383561643835 and parameters: {'subsample': 1.0, 'colsample_bytree': 0.9, 'colsample_bylevel': 1.0, 'min_child_weight': 8, 'max_depth': 12, 'max_delta_step': 2.4000000000000004, 'learning_rate': 0.08662700057393087, 'n_estimators': 106, 'gamma': 15.6}. Best is trial 17 with value: 0.9623287671232876.\u001b[0m\n",
      "\u001b[32m[I 2020-12-04 21:42:09,818]\u001b[0m Trial 12 finished with value: 0.0 and parameters: {'subsample': 0.2, 'colsample_bytree': 0.8, 'colsample_bylevel': 1.0, 'min_child_weight': 11, 'max_depth': 11, 'max_delta_step': 9.8, 'learning_rate': 0.036959697151918595, 'n_estimators': 430, 'gamma': 1.4000000000000001}. Best is trial 17 with value: 0.9623287671232876.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-12-04 21:42:10,471]\u001b[0m Trial 15 finished with value: 0.9657534246575342 and parameters: {'subsample': 0.7000000000000001, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.6, 'min_child_weight': 3, 'max_depth': 10, 'max_delta_step': 5.7, 'learning_rate': 0.015046570095330685, 'n_estimators': 376, 'gamma': 2.4000000000000004}. Best is trial 15 with value: 0.9657534246575342.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "2020-12-04 21:42:10,506 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:147 — Best params found for: XGBClassifier with score: 0.9657534246575342\n",
      "2020-12-04 21:42:10,889 — dermclass_models2.pipeline — INFO —_tune_hyperparameters:150 — Successfully tuned hyperparameters\n"
     ]
    }
   ],
   "source": [
    "model = sm.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.get_modeling_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dermclass_models2.preprocessing import TextPreprocessors\n",
    "from dermclass_models2.config import TextConfig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-02 20:16:53,484 — dermclass_models2.preprocessing — INFO —_load_class_from_dir:272 — Successfully loaded class lichen_planus\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "pp = TextPreprocessors(TextConfig)\n",
    "df = pp._load_class_from_dir(TextConfig.DATA_PATH / \"lichen_planus\")\n",
    "\n",
    "df['encoded_cat'] = df['target'].astype('category').cat.codes\n",
    "\n",
    "data_texts = df[\"text\"].to_list() # Features (not-tokenized yet)\n",
    "data_labels = df[\"encoded_cat\"].to_list() # Lables\n",
    "\n",
    "# Split Train and Validation data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Keep some data for inference (testing)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8c7ba8b6a411>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m raw_train_ds = preprocessing.text_dataset_from_directory(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "raw_train_ds = preprocessing.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 16770, 1024, 1013, 1013, 4372, 1012, 16948, 1012, 8917, 1013, 15536, 3211, 1013, 5622, 8661, 1035, 2933, 2271, 5622, 8661, 2933, 2271, 1006, 6948, 1007, 2003, 1037, 11888, 20187, 1998, 11311, 1011, 19872, 4295, 2008, 13531, 1996, 3096, 1010, 10063, 1010, 2606, 1010, 1998, 14163, 27199, 24972, 1012, 1031, 1015, 1033, 2009, 2003, 7356, 2011, 26572, 20028, 1010, 4257, 1011, 9370, 1010, 10482, 3401, 3560, 6643, 14289, 4244, 1998, 28487, 2007, 15241, 2075, 1010, 2128, 4588, 8898, 1010, 2986, 2317, 4094, 1006, 15536, 3600, 3511, 1005, 1055, 2358, 4360, 2063, 1007, 1010, 4141, 12473, 12759, 2398, 1010, 23951, 11137, 12150, 1998, 27323, 1010, 8260, 1010, 15099, 2896, 3456, 1998, 8700, 14163, 13186, 2050, 1012, 1031, 1016, 1033, 2348, 2045, 2003, 1037, 5041, 6612, 2846, 1997, 6948, 24491, 2015, 1010, 1996, 3096, 1998, 8700, 17790, 3961, 2004, 1996, 2350, 4573, 1997, 6624, 1012, 1031, 1017, 1033, 1996, 3426, 2003, 4242, 1010, 2021, 2009, 2003, 2245, 2000, 2022, 1996, 2765, 1997, 2019, 8285, 5714, 23041, 2063, 2832, 2007, 2019, 4242, 3988, 9495, 1012, 2045, 2003, 2053, 9526, 1010, 2021, 2116, 2367, 20992, 1998, 8853, 2031, 2042, 2109, 1999, 4073, 2000, 2491, 1996, 8030, 1012, 1996, 2744, 5622, 8661, 9314, 4668, 1006, 5622, 8661, 9314, 17259, 2030, 5622, 8661, 9314, 4649, 3258, 1007, 5218, 2000, 1037, 4649, 3258, 1997, 2714, 2030, 7235, 2010, 14399, 8988, 12898, 12863, 1998, 6612, 3311, 2000, 5622, 8661, 2933, 2271, 1006, 1045, 1012, 1041, 1012, 1010, 2019, 2181, 2029, 12950, 5622, 8661, 2933, 2271, 1010, 2119, 2000, 1996, 6248, 3239, 1998, 2104, 1037, 24635, 1007, 1012, 1031, 1018, 1033, 1031, 1019, 1033, 2823, 11394, 4475, 2030, 3056, 20992, 2064, 3426, 1037, 5622, 8661, 9314, 4668, 1012, 1031, 1018, 1033, 2027, 2064, 2036, 5258, 1999, 2523, 2007, 22160, 2102, 6431, 3677, 4295, 1012, 1031, 1018, 1033, 1031, 1020, 1033, 1024, 24398, 8417, 1015, 5579, 1015, 1012, 1015, 2609, 1015, 1012, 1016, 5418, 1015, 1012, 1017, 17702, 8715, 2015, 1016, 5751, 1998, 8030, 1016, 1012, 1015, 3096, 1016, 1012, 1016, 14163, 27199, 24972, 1017, 5320, 1018, 26835, 19009, 1019, 11616, 1019, 1012, 1015, 3096, 1019, 1012, 1016, 2677, 1019, 1012, 1017, 11658, 11616, 1019, 1012, 1018, 2010, 14399, 8988, 6779, 1020, 3949, 1020, 1012, 1015, 3096, 1020, 1012, 1016, 2677, 1021, 4013, 26745, 6190, 1022, 4958, 5178, 4328, 6779, 1023, 2381, 2184, 2470, 2340, 3964, 2260, 7604, 2410, 6327, 6971, 5579, 5622, 8661, 2933, 2271, 22520, 2024, 2061, 2170, 2138, 1997, 2037, 1000, 5622, 8661, 1011, 2066, 1000, 3311, 1031, 1021, 1033, 1998, 2064, 2022, 6219, 2011, 1996, 2609, 2027, 9125, 1010, 2030, 2011, 2037, 19476, 1012, 2609, 5622, 8661, 2933, 2271, 2089, 2022, 20427, 2004, 12473, 14163, 13186, 2389, 2030, 3013, 17191, 9972, 1012, 3013, 17191, 3596, 2024, 2216, 12473, 1996, 3096, 1010, 21065, 1010, 1998, 10063, 1012, 1031, 1022, 1033, 1031, 1023, 1033, 1031, 2184, 1033, 14163, 13186, 2389, 3596, 2024, 2216, 12473, 1996, 14834, 1997, 1996, 3806, 13181, 18447, 19126, 12859, 1006, 2677, 1010, 6887, 5649, 26807, 1010, 9686, 7361, 3270, 12349, 1010, 4308, 1010, 2019, 2271, 1007, 1010, 2474, 18143, 2595, 1010, 1998, 2060, 14163, 13186, 102], [101, 16770, 1024, 1013, 1013, 7479, 1012, 17237, 1012, 2866, 1013, 3785, 1013, 5622, 8661, 1011, 2933, 2271, 1013, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 2003, 1037, 23438, 2008, 2064, 7461, 2367, 3033, 1997, 2115, 2303, 1010, 2164, 1996, 2503, 1997, 2115, 2677, 1012, 2156, 1037, 14246, 2065, 2017, 2228, 2017, 2453, 2031, 2009, 1012, 2512, 1011, 13661, 6040, 1024, 2156, 1037, 14246, 2065, 2017, 2031, 1024, 12906, 1997, 12538, 1010, 2992, 1010, 6379, 1011, 2417, 1038, 10994, 8376, 2006, 2115, 2608, 1010, 3456, 2030, 2303, 1006, 2017, 2089, 2156, 2986, 2317, 3210, 2006, 1996, 1038, 10994, 8376, 1007, 2317, 13864, 2006, 2115, 16031, 2015, 1010, 4416, 2030, 1996, 19008, 1997, 2115, 6029, 5255, 1998, 22748, 1999, 2115, 2677, 1010, 2926, 2043, 2017, 4521, 2030, 4392, 13852, 13864, 6037, 2006, 2115, 21065, 14699, 2417, 13864, 2006, 2115, 24728, 22144, 5931, 1010, 4857, 5582, 10063, 2007, 25880, 2006, 3614, 1011, 5044, 6379, 2030, 2317, 13864, 2006, 2115, 19085, 2122, 2024, 8030, 1997, 5622, 8661, 2933, 2271, 1012, 2017, 2089, 2069, 2031, 1015, 1997, 2122, 8030, 1012, 5622, 8661, 2933, 2271, 2006, 2115, 3096, 2064, 2022, 2200, 2009, 11714, 1010, 2021, 2025, 2467, 1012, 2592, 1024, 21887, 23350, 10651, 1024, 2129, 2000, 3967, 1037, 14246, 2009, 1005, 1055, 2145, 2590, 2000, 2131, 2393, 2013, 1037, 14246, 2065, 2017, 2342, 2009, 1012, 2000, 3967, 2115, 14246, 5970, 1024, 3942, 2037, 4037, 2224, 1996, 17237, 10439, 2655, 2068, 2424, 2041, 2055, 2478, 1996, 17237, 2076, 21887, 23350, 5622, 8661, 2933, 2271, 2006, 1996, 2503, 1997, 1996, 7223, 5622, 8661, 2933, 2271, 2411, 3544, 2006, 1996, 2503, 1997, 2115, 7223, 2317, 13864, 1997, 5622, 8661, 2933, 2271, 1999, 1996, 2677, 2317, 13864, 1999, 2115, 2677, 2089, 2022, 5622, 8661, 2933, 2271, 2065, 2017, 1005, 2128, 2025, 2469, 2009, 1005, 1055, 5622, 8661, 2933, 2271, 13441, 2013, 1037, 14246, 5622, 8661, 2933, 2271, 2006, 2115, 3096, 2788, 4152, 2488, 2006, 2049, 2219, 1999, 2055, 1023, 2000, 2324, 2706, 1012, 6949, 2015, 1998, 1051, 18447, 8163, 2013, 1037, 14246, 2064, 2393, 2491, 1996, 23438, 1998, 7496, 2009, 8450, 1012, 2065, 6949, 2015, 1998, 1051, 18447, 8163, 2079, 2025, 2147, 2030, 2017, 2031, 5729, 5622, 8661, 2933, 2271, 1010, 26261, 22943, 17596, 2030, 3949, 2007, 1037, 2569, 2785, 1997, 2422, 1006, 2422, 7242, 1007, 2064, 2393, 1012, 5622, 8661, 2933, 2271, 1999, 2115, 2677, 2064, 2197, 2005, 2195, 2086, 1012, 2677, 28556, 2229, 1998, 12509, 2015, 2013, 1037, 14246, 2064, 2393, 7496, 8030, 2066, 5255, 2030, 14699, 16031, 2015, 1012, 2017, 3685, 4608, 5622, 8661, 2933, 2271, 1998, 2009, 2515, 2025, 2788, 2272, 2067, 2320, 2009, 1005, 1055, 5985, 2039, 1012, 2005, 2490, 1998, 2592, 1010, 2156, 2866, 5622, 8661, 2933, 2271, 1012, 2129, 2000, 15804, 5622, 8661, 2933, 2271, 2012, 2188, 2065, 2017, 2031, 5622, 8661, 2933, 2271, 2006, 2115, 3096, 1024, 9378, 2007, 5810, 4010, 2300, 1516, 4468, 7815, 2015, 1998, 2303, 9378, 2229, 9378, 2115, 2606, 2058, 1037, 7752, 2030, 7198, 2061, 1996, 25850, 24667, 2515, 2025, 2272, 2046, 3967, 2007, 1996, 2717, 1997, 2115, 3096, 2224, 2019, 7861, 14511, 11638, 1006, 11052, 9496, 102], [101, 16770, 1024, 1013, 1013, 7479, 1012, 14415, 20464, 5498, 2278, 1012, 8917, 1013, 7870, 1011, 3785, 1013, 5622, 8661, 1011, 2933, 2271, 1013, 8030, 1011, 5320, 1013, 25353, 2278, 1011, 18540, 22203, 24434, 2620, 1001, 1024, 1066, 1024, 3793, 1027, 5622, 8661, 1003, 2322, 24759, 20849, 1003, 2322, 1006, 4682, 1003, 14134, 5283, 2078, 1003, 2322, 13068, 1010, 2008, 1003, 2322, 24844, 18349, 2361, 1003, 2322, 7840, 1003, 27074, 22507, 2389, 1003, 2322, 28075, 2015, 1012, 19184, 6302, 1997, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 2330, 3769, 1011, 2039, 13764, 8649, 3482, 8458, 11439, 1997, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 1997, 10063, 2330, 3769, 1011, 2039, 13764, 8649, 3482, 6525, 2140, 5622, 8661, 2933, 2271, 22520, 3426, 19959, 2317, 13864, 1999, 1996, 2677, 1012, 8700, 5622, 8661, 2933, 2271, 2330, 3769, 1011, 2039, 13764, 8649, 3482, 5622, 8661, 2933, 2271, 1006, 4682, 1011, 28919, 2377, 1011, 16371, 2015, 1007, 2003, 1037, 4650, 2008, 2064, 3426, 18348, 1998, 17373, 1999, 1996, 3096, 1010, 2606, 1010, 10063, 1998, 14163, 27199, 24972, 1012, 2006, 1996, 3096, 1010, 5622, 8661, 2933, 2271, 2788, 3544, 2004, 16405, 14536, 13602, 1010, 2009, 11714, 1010, 4257, 18548, 2008, 4503, 2058, 2195, 3134, 1012, 1999, 1996, 2677, 1010, 12436, 20876, 1998, 2060, 2752, 3139, 2011, 1037, 14163, 27199, 10804, 1010, 5622, 8661, 2933, 2271, 3596, 19959, 2317, 13864, 1010, 2823, 2007, 9145, 14699, 2015, 1012, 2087, 2111, 2064, 6133, 5171, 1010, 10256, 3572, 1997, 5622, 8661, 2933, 2271, 2012, 2188, 1010, 2302, 2966, 2729, 1012, 2065, 1996, 4650, 5320, 3255, 2030, 3278, 2009, 8450, 1010, 2017, 2089, 2342, 20422, 5850, 1012, 5622, 8661, 2933, 2271, 3475, 1005, 1056, 9530, 15900, 6313, 1012, 3688, 1004, 2578, 2338, 1024, 14415, 9349, 2155, 2740, 2338, 1010, 4833, 3179, 2265, 2062, 3688, 2013, 14415, 9349, 8030, 1996, 5751, 1998, 8030, 1997, 5622, 8661, 2933, 2271, 8137, 5834, 2006, 1996, 2752, 5360, 1012, 5171, 5751, 1998, 8030, 2024, 1024, 16405, 14536, 13602, 1010, 4257, 18548, 1010, 2087, 2411, 2006, 1996, 5110, 16148, 1010, 7223, 2030, 10792, 1010, 1998, 2823, 1996, 8991, 18400, 2015, 2009, 8450, 1038, 9863, 2545, 2008, 3338, 2000, 2433, 8040, 7875, 2015, 2030, 19116, 2015, 19959, 2317, 13864, 1999, 1996, 2677, 2030, 2006, 1996, 2970, 2030, 4416, 9145, 14699, 2015, 1999, 1996, 2677, 2030, 12436, 20876, 2606, 3279, 2689, 1999, 21065, 3609, 13774, 4053, 2030, 3279, 2043, 2000, 2156, 1037, 3460, 2156, 2115, 3460, 2065, 4714, 18548, 2030, 1037, 23438, 1011, 2066, 4650, 3544, 2006, 2115, 3096, 2005, 2053, 6835, 3114, 1010, 2107, 2004, 1037, 2124, 27395, 4668, 2030, 3967, 2007, 9947, 7768, 1012, 2036, 2156, 2115, 3460, 2065, 2017, 3325, 2151, 5751, 2030, 8030, 3378, 2007, 5622, 8661, 2933, 2271, 1997, 1996, 2677, 1010, 8991, 18400, 2015, 1010, 21065, 2030, 10063, 1012, 2009, 1005, 1055, 2190, 2000, 2131, 1037, 25732, 1998, 8321, 11616, 2138, 1037, 2193, 1997, 3096, 1998, 14163, 13186, 2389, 3785, 2064, 3426, 22520, 1998, 17964, 1012, 5227, 2019, 6098, 2012, 14415, 9349, 5320, 5622, 8661, 2933, 2271, 5158, 2043, 2115, 11311, 2291, 4491, 4442, 1997, 1996, 3096, 2030, 14163, 27199, 24972, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert (PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),) to PyString",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-02c9dc283520>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCustomTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mencode_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, tokenizer_file, do_lower_case, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mtokenize_chinese_chars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenize_chinese_chars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         )\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfast_tokenizer_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;31m# We have a serialization from tokenizers which let us directly build the backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mfast_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizerFast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfast_tokenizer_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't convert (PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),) to PyString"
     ]
    }
   ],
   "source": [
    "class CustomTokenizer(tokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self):\n",
    "        encode_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset, tokenizer):\n",
    "    text_to_encode = []\n",
    "    labels = []\n",
    "    for batch in dataset.as_numpy_iterator():\n",
    "        text_batch = batch[0]\n",
    "        labels_batch = batch[1]\n",
    "        \n",
    "        text_batch_decoded = [text.decode(\"utf-8\") for text in text_batch]\n",
    "        text_to_encode += text_batch_decoded\n",
    "        labels += labels_batch.tolist()\n",
    "    text_encodings = tokenizer(text_to_encode, truncation=True, padding=True)\n",
    "    text_batch_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(text_encodings),\n",
    "        labels))\n",
    "    return text_batch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(tf.keras.layers.Layer):  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def build(self, xxxx):\n",
    "        pass\n",
    "    \n",
    "    def call(self, dataset, tokenizer):\n",
    "        dataset = encode_dataset(dataset, tokenizer)\n",
    "        return dataset\n",
    "layer = MyLayer()\n",
    "z = layer(raw_train_dataset, tokenizer)\n",
    "seq = tf.keras.Sequential([layer,\n",
    "                           model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'> input: <PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "call() missing 1 required positional argument: 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-227-2e61915f6eed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_train_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: call() missing 1 required positional argument: 'tokenizer'"
     ]
    }
   ],
   "source": [
    "seq(raw_train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp = encode_dataset(raw_train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': array([  101, 16770,  1024,  1013,  1013,  7479,  1012,  4773, 26876,\n",
      "        1012,  4012,  1013,  3096,  1011,  3471,  1011,  1998,  1011,\n",
      "       13441,  1013,  8827, 11069,  6190,  1013,  4824,  1011,  8827,\n",
      "       11069,  6190,  1011, 24078,  1001,  1024,  1066,  1024,  3793,\n",
      "        1027,  8827, 11069,  6190,  1003,  2322,  2483,  1003,  2322,\n",
      "        2050,  1003, 27074,  4939,  1003,  2322, 10521,  8551,  2121,\n",
      "        1010,  2979,  1003,  2322, 19699,  5358,  1003,  2322, 27576,\n",
      "        1003,  2322,  3406,  1003,  2322, 27576,  1012,  2054,  2003,\n",
      "        8827, 11069,  6190,  1029,  8827, 11069,  6190,  2003,  1037,\n",
      "        3096,  8761,  2008,  5320,  3096,  4442,  2000,  4800, 22086,\n",
      "        2039,  2000,  2184,  2335,  5514,  2084,  3671,  1012,  2023,\n",
      "        3084,  1996,  3096,  3857,  2039,  2046, 16906,  2100,  2417,\n",
      "       13864,  3139,  2007,  2317,  9539,  1012,  2027,  2064,  4982,\n",
      "        5973,  1010,  2021,  2087,  3711,  2006,  1996, 21065,  1010,\n",
      "       13690,  1010,  5042,  1010,  1998,  2896,  2067,  1012,  8827,\n",
      "       11069,  6190,  2064,  1005,  1056,  2022,  2979,  2013,  2711,\n",
      "        2000,  2711,  1012,  2009,  2515,  2823,  4148,  1999,  2372,\n",
      "        1997,  1996,  2168,  2155,  1012,  8827, 11069,  6190,  2788,\n",
      "        3544,  1999,  2220, 20480,  1012,  2005,  2087,  2111,  1010,\n",
      "        2009, 13531,  2074,  1037,  2261,  2752,  1012,  1999,  5729,\n",
      "        3572,  1010,  8827, 11069,  6190,  2064,  3104,  2312,  3033,\n",
      "        1997,  1996,  2303,  1012,  1996, 13864,  2064, 11005,  1998,\n",
      "        2059,  2272,  2067,  2802,  1037,  2711,  1005,  1055,  2166,\n",
      "        1012,  2131,  1996, 24078,  2006,  8827, 11069,  6190,  8030,\n",
      "        1996,  8030,  1997,  8827, 11069,  6190,  8137,  5834,  2006,\n",
      "        1996,  2828,  2017,  2031,  1012,  2070,  2691,  8030,  2005,\n",
      "       11952,  8827, 11069,  6190,  1011,  1011,  1996,  2087,  2691,\n",
      "        3528,  1997,  1996,  4650,  1011,  1011,  2421,  1024,  8827,\n",
      "       11069,  6190, 24759, 20784, 15808,  1997,  2417,  3096,  1010,\n",
      "        2411,  3139,  2007,  3165,  1011,  6910,  9539,  1012,  2122,\n",
      "       28487,  2089,  2022,  2009, 11714,  1998,  9145,  1010,  1998,\n",
      "        2027,  2823,  8579,  1998, 19501,  1012,  1999,  5729,  3572,\n",
      "        1010,  1996, 28487,  2097,  4982,  1998, 13590,  1010,  5266,\n",
      "        2312,  2752,  1012, 10840,  1997,  1996, 21243,  1998, 11756,\n",
      "       25464,  2015,  1010,  2164, 12532, 10626,  3370,  1998, 15091,\n",
      "        2075,  1997,  1996, 10063,  1012,  1996, 10063,  2089,  2036,\n",
      "       13675, 26607,  2030, 20010,  6776,  2013,  1996, 13774,  2793,\n",
      "        1012, 28487,  1997,  9539,  2030, 19116,  2006,  1996, 21065,\n",
      "        1012,  2111,  2007,  8827, 11069,  6190,  2064,  2036,  2131,\n",
      "        1037,  2828,  1997, 27641,  2170,  8827, 11069,  4588, 27641,\n",
      "        1012,  2009,  5320,  3255,  1998, 18348,  1999,  1996, 17651,\n",
      "        1012,  1996,  2120,  8827, 11069,  6190,  3192, 10035,  2008,\n",
      "        2090,  2184,  1003,  2000,  2382,  1003,  1997,  2111,  2007,\n",
      "        8827, 11069,  6190,  2036,  2031,  8827, 11069,  4588, 27641,\n",
      "        1012,  4127,  2060,  4127,  1997,  8827, 11069,  6190,  2421,\n",
      "        1024, 16405,  3367,  7934,  8827, 11069,  6190,  1010,  2029,\n",
      "        5320,  2417,  1998,  8040, 20766,  3096,  2007,  4714, 16405,\n",
      "        3367, 16308,  2006,  1996,  9486,  1997,  1996,  2398,  1998,\n",
      "        7082,  2015,  1997,  1996,  2519,  1012,  9535, 12259,  8827,\n",
      "       11069,  6190,  1010,  2029,  2411,  4627,  1999,  5593,  2030,\n",
      "        2402, 20480,  1010,  5320,  2235,  1010,  2417,  7516,  1010,\n",
      "        3701,  2006,  1996, 15190,  1998, 10726,  1012, 27099,  2089,\n",
      "        2022, 16464, 15245,  1010,  2358,  2890,  2361,  3759,  1010,\n",
      "        6197,  8591, 13706,  1010,  6911,  1010,  4544,  2000,  1996,\n",
      "        3096,  1010,  1998,  2635,  3424,  9067, 10980,  2140,  1998,\n",
      "        8247,  1011,  3796,  2121, 20992,  1012, 19262,  8827, 11069,\n",
      "        6190,  1010,  2029,  3084,  4408,  2417,  1010,   102]), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1])}, 1)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset.as_numpy_iterator():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 16770, 1024, 1013, 1013, 7479, 1012, 4773, 26876, 1012, 4012, 1013, 3096, 1011, 3471, 1011, 1998, 1011, 13441, 1013, 8827, 11069, 6190, 1013, 4824, 1011, 8827, 11069, 6190, 1011, 24078, 1001, 1024, 1066, 1024, 3793, 1027, 8827, 11069, 6190, 1003, 2322, 2483, 1003, 2322, 2050, 1003, 27074, 4939, 1003, 2322, 10521, 8551, 2121, 1010, 2979, 1003, 2322, 19699, 5358, 1003, 2322, 27576, 1003, 2322, 3406, 1003, 2322, 27576, 1012, 2054, 2003, 8827, 11069, 6190, 1029, 8827, 11069, 6190, 2003, 1037, 3096, 8761, 2008, 5320, 3096, 4442, 2000, 4800, 22086, 2039, 2000, 2184, 2335, 5514, 2084, 3671, 1012, 2023, 3084, 1996, 3096, 3857, 2039, 2046, 16906, 2100, 2417, 13864, 3139, 2007, 2317, 9539, 1012, 2027, 2064, 4982, 5973, 1010, 2021, 2087, 3711, 2006, 1996, 21065, 1010, 13690, 1010, 5042, 1010, 1998, 2896, 2067, 1012, 8827, 11069, 6190, 2064, 1005, 1056, 2022, 2979, 2013, 2711, 2000, 2711, 1012, 2009, 2515, 2823, 4148, 1999, 2372, 1997, 1996, 2168, 2155, 1012, 8827, 11069, 6190, 2788, 3544, 1999, 2220, 20480, 1012, 2005, 2087, 2111, 1010, 2009, 13531, 2074, 1037, 2261, 2752, 1012, 1999, 5729, 3572, 1010, 8827, 11069, 6190, 2064, 3104, 2312, 3033, 1997, 1996, 2303, 1012, 1996, 13864, 2064, 11005, 1998, 2059, 2272, 2067, 2802, 1037, 2711, 1005, 1055, 2166, 1012, 2131, 1996, 24078, 2006, 8827, 11069, 6190, 8030, 1996, 8030, 1997, 8827, 11069, 6190, 8137, 5834, 2006, 1996, 2828, 2017, 2031, 1012, 2070, 2691, 8030, 2005, 11952, 8827, 11069, 6190, 1011, 1011, 1996, 2087, 2691, 3528, 1997, 1996, 4650, 1011, 1011, 2421, 1024, 8827, 11069, 6190, 24759, 20784, 15808, 1997, 2417, 3096, 1010, 2411, 3139, 2007, 3165, 1011, 6910, 9539, 1012, 2122, 28487, 2089, 2022, 2009, 11714, 1998, 9145, 1010, 1998, 2027, 2823, 8579, 1998, 19501, 1012, 1999, 5729, 3572, 1010, 1996, 28487, 2097, 4982, 1998, 13590, 1010, 5266, 2312, 2752, 1012, 10840, 1997, 1996, 21243, 1998, 11756, 25464, 2015, 1010, 2164, 12532, 10626, 3370, 1998, 15091, 2075, 1997, 1996, 10063, 1012, 1996, 10063, 2089, 2036, 13675, 26607, 2030, 20010, 6776, 2013, 1996, 13774, 2793, 1012, 28487, 1997, 9539, 2030, 19116, 2006, 1996, 21065, 1012, 2111, 2007, 8827, 11069, 6190, 2064, 2036, 2131, 1037, 2828, 1997, 27641, 2170, 8827, 11069, 4588, 27641, 1012, 2009, 5320, 3255, 1998, 18348, 1999, 1996, 17651, 1012, 1996, 2120, 8827, 11069, 6190, 3192, 10035, 2008, 2090, 2184, 1003, 2000, 2382, 1003, 1997, 2111, 2007, 8827, 11069, 6190, 2036, 2031, 8827, 11069, 4588, 27641, 1012, 4127, 2060, 4127, 1997, 8827, 11069, 6190, 2421, 1024, 16405, 3367, 7934, 8827, 11069, 6190, 1010, 2029, 5320, 2417, 1998, 8040, 20766, 3096, 2007, 4714, 16405, 3367, 16308, 2006, 1996, 9486, 1997, 1996, 2398, 1998, 7082, 2015, 1997, 1996, 2519, 1012, 9535, 12259, 8827, 11069, 6190, 1010, 2029, 2411, 4627, 1999, 5593, 2030, 2402, 20480, 1010, 5320, 2235, 1010, 2417, 7516, 1010, 3701, 2006, 1996, 15190, 1998, 10726, 1012, 27099, 2089, 2022, 16464, 15245, 1010, 2358, 2890, 2361, 3759, 1010, 6197, 8591, 13706, 1010, 6911, 1010, 4544, 2000, 1996, 3096, 1010, 1998, 2635, 3424, 9067, 10980, 2140, 1998, 8247, 1011, 3796, 2121, 20992, 1012, 19262, 8827, 11069, 6190, 1010, 2029, 3084, 4408, 2417, 1010, 102], [101, 16770, 1024, 1013, 1013, 4315, 2213, 7159, 14191, 1012, 8917, 1013, 7832, 1013, 5622, 8661, 1011, 2933, 2271, 1013, 2054, 2003, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2003, 1037, 11888, 20187, 3096, 4650, 12473, 1996, 3096, 1998, 14163, 13186, 2389, 9972, 1012, 2045, 2024, 2195, 6612, 4127, 1997, 5622, 8661, 2933, 2271, 2008, 3745, 2714, 2838, 2006, 2010, 14399, 8988, 6779, 1012, 3013, 17191, 5622, 8661, 2933, 2271, 14163, 13186, 2389, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 7361, 11733, 6935, 5622, 8661, 2933, 2271, 1997, 1996, 10063, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 5622, 8661, 9314, 4319, 17259, 2040, 4152, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 13531, 2055, 2028, 1999, 2028, 3634, 2111, 4969, 1010, 3262, 12473, 6001, 2058, 1996, 2287, 1997, 2871, 2086, 1012, 2055, 2431, 2216, 5360, 2031, 8700, 5622, 8661, 2933, 2271, 1010, 2029, 2003, 2062, 2691, 1999, 2308, 2084, 1999, 2273, 1012, 2055, 2184, 1003, 2031, 5622, 8661, 2933, 2271, 1997, 1996, 10063, 1012, 2054, 5320, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2003, 1037, 1056, 3526, 1011, 19872, 8285, 5714, 23041, 2063, 8761, 1010, 1999, 2029, 20187, 4442, 2886, 2019, 4242, 5250, 2306, 1996, 3096, 1998, 14163, 13186, 2389, 17710, 8609, 5740, 27321, 1012, 8020, 5876, 2000, 5622, 8661, 2933, 2271, 2089, 2421, 1024, 7403, 3653, 10521, 26994, 3558, 1998, 6832, 6911, 4544, 2000, 1996, 3096, 1025, 5622, 8661, 2933, 2271, 2411, 3544, 2073, 1996, 3096, 2038, 2042, 15047, 2030, 2044, 5970, 1517, 2023, 2003, 2170, 1996, 11163, 18078, 3433, 1006, 12849, 15878, 3678, 6648, 1007, 2334, 5084, 3096, 4295, 2107, 2004, 2014, 10374, 1062, 14122, 2121, 1517, 11163, 14399, 2594, 3433, 22575, 13434, 8985, 1010, 2107, 2004, 28389, 1039, 1006, 2029, 2453, 19933, 2969, 1011, 28873, 2015, 2006, 1996, 3302, 1997, 15191, 17710, 8609, 5740, 27321, 1007, 3967, 2035, 24395, 1010, 2107, 2004, 2000, 3384, 8110, 2015, 1999, 8700, 5622, 8661, 2933, 2271, 1006, 4678, 1007, 5850, 1025, 2751, 1010, 21864, 19105, 1010, 21864, 3490, 10672, 1998, 2500, 2064, 3426, 1037, 5622, 8661, 9314, 23438, 1012, 1037, 5622, 8661, 9314, 21733, 2003, 2036, 3862, 1999, 22160, 2102, 1011, 6431, 1011, 3677, 4295, 1010, 1037, 4012, 21557, 1997, 1037, 5923, 24960, 22291, 1012, 2054, 2024, 1996, 6612, 2838, 1997, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2089, 3426, 1037, 2235, 2193, 2030, 2116, 22520, 2006, 1996, 3096, 1998, 14163, 13186, 2389, 9972, 1012, 3013, 17191, 5622, 8661, 2933, 2271, 1996, 5156, 8312, 1997, 1996, 4295, 2003, 4556, 5622, 8661, 2933, 2271, 1012, 8030, 2064, 2846, 2013, 3904, 1006, 13191, 1007, 2000, 6387, 2009, 2818, 1012, 6643, 14289, 4244, 1998, 26572, 20028, 28487, 2024, 12538, 1010, 4257, 1011, 9370, 1998, 3813, 2006, 14412, 24952, 2239, 1012, 1996, 28487, 2024, 4625, 2011, 2986, 2317, 3210, 2170, 15536, 3600, 3511, 2358, 4360, 2063, 1012, 23760, 13181, 17926, 5622, 8661, 2933, 2271, 2064, 2022, 8040, 20766, 1012, 2012, 18981, 16066, 5622, 8661, 2933, 2271, 2003, 1037, 4678, 5754, 7934, 8349, 2007, 2019, 2012, 18981, 16066, 2803, 1012, 7087, 3560, 5622, 8661, 2933, 2271, 2003, 4678, 1012, 2946, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first argument to `Layer.call` must always be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-da0fbe935072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizingLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;31m#   not to any other argument.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[1;31m# - setting the SavedModel saving spec.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_out_first_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_split_out_first_arg\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2978\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m       raise ValueError(\n\u001b[1;32m-> 2980\u001b[1;33m           'The first argument to `Layer.call` must always be passed.')\n\u001b[0m\u001b[0;32m   2981\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The first argument to `Layer.call` must always be passed."
     ]
    }
   ],
   "source": [
    "tl = TokenizingLayer()\n",
    "tl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 16770, 1024, 1013, 1013, 4315, 2213, 7159, 14191, 1012, 8917, 1013, 7832, 1013, 5622, 8661, 1011, 2933, 2271, 1013, 2054, 2003, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2003, 1037, 11888, 20187, 3096, 4650, 12473, 1996, 3096, 1998, 14163, 13186, 2389, 9972, 1012, 2045, 2024, 2195, 6612, 4127, 1997, 5622, 8661, 2933, 2271, 2008, 3745, 2714, 2838, 2006, 2010, 14399, 8988, 6779, 1012, 3013, 17191, 5622, 8661, 2933, 2271, 14163, 13186, 2389, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 7361, 11733, 6935, 5622, 8661, 2933, 2271, 1997, 1996, 10063, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 5622, 8661, 9314, 4319, 17259, 2040, 4152, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 13531, 2055, 2028, 1999, 2028, 3634, 2111, 4969, 1010, 3262, 12473, 6001, 2058, 1996, 2287, 1997, 2871, 2086, 1012, 2055, 2431, 2216, 5360, 2031, 8700, 5622, 8661, 2933, 2271, 1010, 2029, 2003, 2062, 2691, 1999, 2308, 2084, 1999, 2273, 1012, 2055, 2184, 1003, 2031, 5622, 8661, 2933, 2271, 1997, 1996, 10063, 1012, 2054, 5320, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2003, 1037, 1056, 3526, 1011, 19872, 8285, 5714, 23041, 2063, 8761, 1010, 1999, 2029, 20187, 4442, 2886, 2019, 4242, 5250, 2306, 1996, 3096, 1998, 14163, 13186, 2389, 17710, 8609, 5740, 27321, 1012, 8020, 5876, 2000, 5622, 8661, 2933, 2271, 2089, 2421, 1024, 7403, 3653, 10521, 26994, 3558, 1998, 6832, 6911, 4544, 2000, 1996, 3096, 1025, 5622, 8661, 2933, 2271, 2411, 3544, 2073, 1996, 3096, 2038, 2042, 15047, 2030, 2044, 5970, 1517, 2023, 2003, 2170, 1996, 11163, 18078, 3433, 1006, 12849, 15878, 3678, 6648, 1007, 2334, 5084, 3096, 4295, 2107, 2004, 2014, 10374, 1062, 14122, 2121, 1517, 11163, 14399, 2594, 3433, 22575, 13434, 8985, 1010, 2107, 2004, 28389, 1039, 1006, 2029, 2453, 19933, 2969, 1011, 28873, 2015, 2006, 1996, 3302, 1997, 15191, 17710, 8609, 5740, 27321, 1007, 3967, 2035, 24395, 1010, 2107, 2004, 2000, 3384, 8110, 2015, 1999, 8700, 5622, 8661, 2933, 2271, 1006, 4678, 1007, 5850, 1025, 2751, 1010, 21864, 19105, 1010, 21864, 3490, 10672, 1998, 2500, 2064, 3426, 1037, 5622, 8661, 9314, 23438, 1012, 1037, 5622, 8661, 9314, 21733, 2003, 2036, 3862, 1999, 22160, 2102, 1011, 6431, 1011, 3677, 4295, 1010, 1037, 4012, 21557, 1997, 1037, 5923, 24960, 22291, 1012, 2054, 2024, 1996, 6612, 2838, 1997, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2089, 3426, 1037, 2235, 2193, 2030, 2116, 22520, 2006, 1996, 3096, 1998, 14163, 13186, 2389, 9972, 1012, 3013, 17191, 5622, 8661, 2933, 2271, 1996, 5156, 8312, 1997, 1996, 4295, 2003, 4556, 5622, 8661, 2933, 2271, 1012, 8030, 2064, 2846, 2013, 3904, 1006, 13191, 1007, 2000, 6387, 2009, 2818, 1012, 6643, 14289, 4244, 1998, 26572, 20028, 28487, 2024, 12538, 1010, 4257, 1011, 9370, 1998, 3813, 2006, 14412, 24952, 2239, 1012, 1996, 28487, 2024, 4625, 2011, 2986, 2317, 3210, 2170, 15536, 3600, 3511, 2358, 4360, 2063, 1012, 23760, 13181, 17926, 5622, 8661, 2933, 2271, 2064, 2022, 8040, 20766, 1012, 2012, 18981, 16066, 5622, 8661, 2933, 2271, 2003, 1037, 4678, 5754, 7934, 8349, 2007, 2019, 2012, 18981, 16066, 2803, 1012, 7087, 3560, 5622, 8661, 2933, 2271, 2003, 4678, 1012, 2946, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in raw_train_dataset.as_numpy_iterator():\n",
    "    raw_batch = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"a\"\n",
    "def test():\n",
    "    return x\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_decoded = [x.decode(\"utf-8\") for x in raw_batch[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 16770, 1024, 1013, 1013, 7479, 1012, 4773, 26876, 1012, 4012, 1013, 3096, 1011, 3471, 1011, 1998, 1011, 13441, 1013, 8827, 11069, 6190, 1013, 4824, 1011, 8827, 11069, 6190, 1011, 24078, 1001, 1024, 1066, 1024, 3793, 1027, 8827, 11069, 6190, 1003, 2322, 2483, 1003, 2322, 2050, 1003, 27074, 4939, 1003, 2322, 10521, 8551, 2121, 1010, 2979, 1003, 2322, 19699, 5358, 1003, 2322, 27576, 1003, 2322, 3406, 1003, 2322, 27576, 1012, 2054, 2003, 8827, 11069, 6190, 1029, 8827, 11069, 6190, 2003, 1037, 3096, 8761, 2008, 5320, 3096, 4442, 2000, 4800, 22086, 2039, 2000, 2184, 2335, 5514, 2084, 3671, 1012, 2023, 3084, 1996, 3096, 3857, 2039, 2046, 16906, 2100, 2417, 13864, 3139, 2007, 2317, 9539, 1012, 2027, 2064, 4982, 5973, 1010, 2021, 2087, 3711, 2006, 1996, 21065, 1010, 13690, 1010, 5042, 1010, 1998, 2896, 2067, 1012, 8827, 11069, 6190, 2064, 1005, 1056, 2022, 2979, 2013, 2711, 2000, 2711, 1012, 2009, 2515, 2823, 4148, 1999, 2372, 1997, 1996, 2168, 2155, 1012, 8827, 11069, 6190, 2788, 3544, 1999, 2220, 20480, 1012, 2005, 2087, 2111, 1010, 2009, 13531, 2074, 1037, 2261, 2752, 1012, 1999, 5729, 3572, 1010, 8827, 11069, 6190, 2064, 3104, 2312, 3033, 1997, 1996, 2303, 1012, 1996, 13864, 2064, 11005, 1998, 2059, 2272, 2067, 2802, 1037, 2711, 1005, 1055, 2166, 1012, 2131, 1996, 24078, 2006, 8827, 11069, 6190, 8030, 1996, 8030, 1997, 8827, 11069, 6190, 8137, 5834, 2006, 1996, 2828, 2017, 2031, 1012, 2070, 2691, 8030, 2005, 11952, 8827, 11069, 6190, 1011, 1011, 1996, 2087, 2691, 3528, 1997, 1996, 4650, 1011, 1011, 2421, 1024, 8827, 11069, 6190, 24759, 20784, 15808, 1997, 2417, 3096, 1010, 2411, 3139, 2007, 3165, 1011, 6910, 9539, 1012, 2122, 28487, 2089, 2022, 2009, 11714, 1998, 9145, 1010, 1998, 2027, 2823, 8579, 1998, 19501, 1012, 1999, 5729, 3572, 1010, 1996, 28487, 2097, 4982, 1998, 13590, 1010, 5266, 2312, 2752, 1012, 10840, 1997, 1996, 21243, 1998, 11756, 25464, 2015, 1010, 2164, 12532, 10626, 3370, 1998, 15091, 2075, 1997, 1996, 10063, 1012, 1996, 10063, 2089, 2036, 13675, 26607, 2030, 20010, 6776, 2013, 1996, 13774, 2793, 1012, 28487, 1997, 9539, 2030, 19116, 2006, 1996, 21065, 1012, 2111, 2007, 8827, 11069, 6190, 2064, 2036, 2131, 1037, 2828, 1997, 27641, 2170, 8827, 11069, 4588, 27641, 1012, 2009, 5320, 3255, 1998, 18348, 1999, 1996, 17651, 1012, 1996, 2120, 8827, 11069, 6190, 3192, 10035, 2008, 2090, 2184, 1003, 2000, 2382, 1003, 1997, 2111, 2007, 8827, 11069, 6190, 2036, 2031, 8827, 11069, 4588, 27641, 1012, 4127, 2060, 4127, 1997, 8827, 11069, 6190, 2421, 1024, 16405, 3367, 7934, 8827, 11069, 6190, 1010, 2029, 5320, 2417, 1998, 8040, 20766, 3096, 2007, 4714, 16405, 3367, 16308, 2006, 1996, 9486, 1997, 1996, 2398, 1998, 7082, 2015, 1997, 1996, 2519, 1012, 9535, 12259, 8827, 11069, 6190, 1010, 2029, 2411, 4627, 1999, 5593, 2030, 2402, 20480, 1010, 5320, 2235, 1010, 2417, 7516, 1010, 3701, 2006, 1996, 15190, 1998, 10726, 1012, 27099, 2089, 2022, 16464, 15245, 1010, 2358, 2890, 2361, 3759, 1010, 6197, 8591, 13706, 1010, 6911, 1010, 4544, 2000, 1996, 3096, 1010, 1998, 2635, 3424, 9067, 10980, 2140, 1998, 8247, 1011, 3796, 2121, 20992, 1012, 19262, 8827, 11069, 6190, 1010, 2029, 3084, 4408, 2417, 1010, 12538, 22520, 2008, 3711, 1999, 3096, 15439, 1010, 2107, 2004, 1996, 2849, 23270, 2015, 1010, 20092, 1010, 1998, 2104, 1996, 12682, 1012, 9413, 22123, 8093, 27381, 7712, 8827, 11069, 6190, 1010, 2029, 5320, 15443, 2417, 2791, 1997, 1996, 3096, 1998, 8328, 4667, 1997, 9539, 1999, 8697, 1012, 2009, 1005, 1055, 13330, 2011, 5729, 3103, 8022, 1010, 15245, 1010, 3056, 20992, 1010, 1998, 7458, 2070, 7957, 1997, 8827, 11069, 6190, 3949, 1012, 2009, 3791, 2000, 2022, 5845, 3202, 2138, 2009, 2064, 2599, 2000, 5729, 7355, 1012, 4081, 2054, 5320, 8827, 11069, 6190, 1029, 2053, 2028, 4282, 1996, 6635, 3426, 1997, 8827, 11069, 6190, 1010, 2021, 8519, 2903, 2008, 2009, 1521, 1055, 1037, 5257, 1997, 2477, 1012, 2242, 3308, 2007, 1996, 11311, 2291, 5320, 21733, 1010, 29170, 2047, 3096, 4442, 2000, 2433, 2205, 2855, 1012, 5373, 1010, 3096, 4442, 2024, 2999, 2296, 2184, 2000, 2382, 2420, 1012, 2007, 8827, 11069, 6190, 1010, 2047, 4442, 4982, 2296, 1017, 2000, 1018, 2420, 1012, 1996, 3857, 6279, 1997, 2214, 4442, 2108, 2999, 2011, 2047, 3924, 9005, 2216, 3165, 9539, 1012, 8827, 11069, 6190, 12102, 2000, 2448, 1999, 2945, 1010, 2021, 2009, 2089, 2022, 13558, 8213, 1012, 2005, 6013, 1010, 1037, 5615, 1998, 2010, 7631, 2089, 2022, 5360, 1010, 2021, 2025, 1996, 2775, 1005, 1055, 2388, 1012, 2477, 2008, 2064, 9495, 2019, 8293, 1997, 8827, 11069, 6190, 2421, 1024, 7659, 1010, 26988, 2015, 1010, 2030, 5970, 6832, 6911, 2358, 2890, 2361, 15245, 20992, 1010, 2164, 2668, 3778, 20992, 1006, 2066, 8247, 1011, 3796, 2545, 1007, 18479, 18037, 2818, 10626, 2080, 12519, 2063, 1010, 3424, 9067, 10980, 2140, 14667, 11616, 3558, 11360, 1012, 2009, 1521, 1055, 2788, 3733, 2005, 2115, 3460, 2000, 22939, 26745, 3366, 8827, 11069, 6190, 1010, 2926, 2065, 2017, 2031, 28487, 2006, 2752, 2107, 2004, 2115, 1024, 21065, 5551, 13690, 5042, 7579, 6462, 10063, 2115, 3460, 2097, 2507, 2017, 1037, 2440, 3558, 11360, 1998, 3198, 2065, 2111, 1999, 2115, 2155, 2031, 8827, 11069, 6190, 1012, 6845, 5852, 1012, 1996, 3460, 2453, 2079, 1037, 16012, 18075, 1011, 1011, 6366, 1037, 2235, 3538, 1997, 3096, 1998, 3231, 2009, 2000, 2191, 2469, 2017, 2123, 1521, 1056, 2031, 1037, 3096, 8985, 1012, 2045, 1521, 1055, 2053, 2060, 3231, 2000, 12210, 2030, 3627, 2041, 8827, 11069, 6190, 1012, 3949, 15798, 1010, 2045, 2024, 2116, 13441, 1012, 2070, 4030, 1996, 3930, 1997, 2047, 3096, 4442, 1010, 1998, 2500, 15804, 2009, 8450, 1998, 4318, 3096, 1012, 2115, 3460, 2097, 7276, 1037, 3949, 2933, 2008, 2003, 2157, 2005, 2017, 2241, 2006, 1996, 2946, 1997, 2115, 23438, 1010, 2073, 2009, 2003, 2006, 2115, 2303, 1010, 2115, 2287, 1010, 2115, 3452, 2740, 1010, 1998, 2060, 2477, 1012, 2691, 13441, 2421, 1024, 26261, 22943, 6949, 2015, 11052, 9496, 16750, 2005, 4318, 3096, 5317, 16985, 1006, 1037, 2691, 3949, 2005, 21065, 8827, 11069, 6190, 2800, 1999, 2843, 8496, 1010, 6949, 2015, 1010, 17952, 2015, 1010, 25850, 24667, 2015, 1010, 1998, 7198, 7300, 1007, 6949, 2030, 1051, 18447, 3672, 1006, 1037, 2844, 2785, 3641, 2011, 2115, 3460, 1012, 17663, 1040, 1999, 9440, 1998, 15345, 2038, 2053, 3466, 1012, 1007, 2128, 25690, 3593, 6949, 2015, 13441, 2005, 8777, 2000, 5729, 8827, 11069, 6190, 2421, 1024, 2422, 7242, 1012, 1037, 3460, 12342, 2015, 26299, 2422, 2006, 2115, 3096, 2000, 4030, 1996, 3930, 1997, 3096, 4442, 1012, 16405, 3567, 2003, 1037, 3949, 2008, 13585, 1037, 4200, 2170, 8827, 6525, 7770, 2007, 1037, 2569, 2433, 1997, 26299, 2422, 1012, 2777, 12326, 2890, 18684, 2618, 1012, 2023, 4319, 2064, 3426, 5923, 24960, 1998, 11290, 4295, 2004, 2092, 2004, 11192, 3471, 1010, 2061, 2009, 1521, 1055, 2069, 2005, 3809, 3572, 1012, 7435, 4876, 3422, 5022, 1012, 2017, 2097, 2031, 2000, 2131, 6845, 5852, 1010, 3383, 1037, 3108, 1060, 1011, 4097, 1010, 1998, 4298, 1037, 11290, 16012, 18075, 1012, 2128, 25690, 9821, 1012, 2122, 15345, 1010, 6949, 2015, 1010, 17952, 2015, 1010, 2843, 8496, 1010, 1998, 21500, 2015, 2024, 1037, 2465, 1997, 5850, 3141, 2000, 17663, 1037, 1012, 2128, 25690, 9821, 2064, 3426, 3809, 2217, 3896, 1010, 2164, 4182, 18419, 1010, 2061, 2027, 1521, 2128, 2025, 6749, 2005, 2308, 2040, 2024, 6875, 2030, 4041, 2000, 2031, 2336, 1012, 1012, 2023, 4319, 1010, 2081, 2000, 16081, 1996, 11311, 2291, 1010, 2089, 2022, 2579, 2005, 3809, 3572, 2008, 2079, 2025, 6869, 2000, 2060, 13441, 1012, 2009, 2064, 4053, 1996, 14234, 2015, 1998, 5333, 2668, 3778, 1010, 2061, 2115, 3460, 2097, 4876, 3422, 2115, 2740, 2096, 2017, 2202, 2009, 1012, 16012, 27179, 13441, 1012, 2122, 2147, 2011, 10851, 1996, 2303, 1005, 1055, 11311, 2291, 1006, 2029, 2003, 2058, 19620, 1999, 8827, 11069, 6190, 1007, 2000, 2488, 2491, 1996, 21733, 2013, 8827, 11069, 6190, 1012, 16012, 27179, 20992, 2421, 15262, 17960, 12248, 2497, 1006, 14910, 7895, 1007, 1010, 22953, 9305, 12248, 2497, 1006, 9033, 3669, 4160, 1007, 1010, 8292, 5339, 10893, 9759, 2863, 2497, 25039, 4747, 1006, 25022, 2213, 12871, 1007, 27859, 3678, 3401, 13876, 1006, 4372, 13578, 2140, 1007, 1010, 12670, 2884, 5283, 2863, 2497, 1006, 29461, 2213, 12031, 2050, 1007, 1010, 1999, 10258, 7646, 9581, 2497, 1006, 2128, 7712, 9648, 1007, 1010, 11814, 26576, 9759, 2863, 2497, 1006, 21368, 5753, 1007, 1010, 15544, 8791, 3211, 9759, 2863, 2497, 1011, 1054, 4143, 2050, 1006, 3712, 21885, 2072, 1007, 1010, 10819, 14228, 19172, 7875, 1006, 2522, 5054, 3723, 2595, 1007, 1010, 18681, 7265, 3211, 9759, 2863, 2497, 1006, 6335, 2819, 3148, 1007, 1010, 1998, 2149, 23125, 2378, 12248, 2497, 1006, 26261, 8017, 2050, 1007, 1012, 2019, 9007, 24054, 1012, 1996, 14667, 19804, 23238, 8523, 2102, 1006, 27178, 9351, 2721, 1007, 2003, 1037, 2047, 2785, 1997, 4319, 2005, 2146, 1011, 2744, 20187, 7870, 2066, 8827, 11069, 6190, 1998, 8827, 11069, 4588, 27641, 1012, 2009, 1005, 1055, 1037, 17357, 2008, 5991, 1037, 3563, 9007, 1010, 2029, 7126, 2000, 4030, 2060, 9597, 2008, 2599, 2000, 21733, 1012, 102], [101, 16770, 1024, 1013, 1013, 4315, 2213, 7159, 14191, 1012, 8917, 1013, 7832, 1013, 5622, 8661, 1011, 2933, 2271, 1013, 2054, 2003, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2003, 1037, 11888, 20187, 3096, 4650, 12473, 1996, 3096, 1998, 14163, 13186, 2389, 9972, 1012, 2045, 2024, 2195, 6612, 4127, 1997, 5622, 8661, 2933, 2271, 2008, 3745, 2714, 2838, 2006, 2010, 14399, 8988, 6779, 1012, 3013, 17191, 5622, 8661, 2933, 2271, 14163, 13186, 2389, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 7361, 11733, 6935, 5622, 8661, 2933, 2271, 1997, 1996, 10063, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 5622, 8661, 9314, 4319, 17259, 2040, 4152, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 13531, 2055, 2028, 1999, 2028, 3634, 2111, 4969, 1010, 3262, 12473, 6001, 2058, 1996, 2287, 1997, 2871, 2086, 1012, 2055, 2431, 2216, 5360, 2031, 8700, 5622, 8661, 2933, 2271, 1010, 2029, 2003, 2062, 2691, 1999, 2308, 2084, 1999, 2273, 1012, 2055, 2184, 1003, 2031, 5622, 8661, 2933, 2271, 1997, 1996, 10063, 1012, 2054, 5320, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2003, 1037, 1056, 3526, 1011, 19872, 8285, 5714, 23041, 2063, 8761, 1010, 1999, 2029, 20187, 4442, 2886, 2019, 4242, 5250, 2306, 1996, 3096, 1998, 14163, 13186, 2389, 17710, 8609, 5740, 27321, 1012, 8020, 5876, 2000, 5622, 8661, 2933, 2271, 2089, 2421, 1024, 7403, 3653, 10521, 26994, 3558, 1998, 6832, 6911, 4544, 2000, 1996, 3096, 1025, 5622, 8661, 2933, 2271, 2411, 3544, 2073, 1996, 3096, 2038, 2042, 15047, 2030, 2044, 5970, 1517, 2023, 2003, 2170, 1996, 11163, 18078, 3433, 1006, 12849, 15878, 3678, 6648, 1007, 2334, 5084, 3096, 4295, 2107, 2004, 2014, 10374, 1062, 14122, 2121, 1517, 11163, 14399, 2594, 3433, 22575, 13434, 8985, 1010, 2107, 2004, 28389, 1039, 1006, 2029, 2453, 19933, 2969, 1011, 28873, 2015, 2006, 1996, 3302, 1997, 15191, 17710, 8609, 5740, 27321, 1007, 3967, 2035, 24395, 1010, 2107, 2004, 2000, 3384, 8110, 2015, 1999, 8700, 5622, 8661, 2933, 2271, 1006, 4678, 1007, 5850, 1025, 2751, 1010, 21864, 19105, 1010, 21864, 3490, 10672, 1998, 2500, 2064, 3426, 1037, 5622, 8661, 9314, 23438, 1012, 1037, 5622, 8661, 9314, 21733, 2003, 2036, 3862, 1999, 22160, 2102, 1011, 6431, 1011, 3677, 4295, 1010, 1037, 4012, 21557, 1997, 1037, 5923, 24960, 22291, 1012, 2054, 2024, 1996, 6612, 2838, 1997, 5622, 8661, 2933, 2271, 1029, 5622, 8661, 2933, 2271, 2089, 3426, 1037, 2235, 2193, 2030, 2116, 22520, 2006, 1996, 3096, 1998, 14163, 13186, 2389, 9972, 1012, 3013, 17191, 5622, 8661, 2933, 2271, 1996, 5156, 8312, 1997, 1996, 4295, 2003, 4556, 5622, 8661, 2933, 2271, 1012, 8030, 2064, 2846, 2013, 3904, 1006, 13191, 1007, 2000, 6387, 2009, 2818, 1012, 6643, 14289, 4244, 1998, 26572, 20028, 28487, 2024, 12538, 1010, 4257, 1011, 9370, 1998, 3813, 2006, 14412, 24952, 2239, 1012, 1996, 28487, 2024, 4625, 2011, 2986, 2317, 3210, 2170, 15536, 3600, 3511, 2358, 4360, 2063, 1012, 23760, 13181, 17926, 5622, 8661, 2933, 2271, 2064, 2022, 8040, 20766, 1012, 2012, 18981, 16066, 5622, 8661, 2933, 2271, 2003, 1037, 4678, 5754, 7934, 8349, 2007, 2019, 2012, 18981, 16066, 2803, 1012, 7087, 3560, 5622, 8661, 2933, 2271, 2003, 4678, 1012, 2946, 8483, 2013, 9231, 8400, 2000, 3469, 2084, 1037, 9358, 14428, 7913, 1012, 4353, 2089, 2022, 7932, 1010, 25221, 1010, 7399, 1010, 5754, 7934, 2030, 2552, 5498, 2278, 1006, 3103, 1011, 6086, 4573, 2107, 2004, 2227, 1010, 3300, 1998, 10457, 1997, 1996, 2398, 1007, 1012, 3295, 2064, 2022, 5973, 1010, 2021, 2087, 2411, 2392, 1997, 1996, 12150, 1010, 2896, 2067, 1010, 1998, 15392, 1012, 6120, 9041, 2006, 1996, 5776, 1521, 1055, 3096, 2828, 1012, 2047, 6643, 14289, 4244, 1998, 28487, 2411, 2031, 1037, 6379, 2030, 8766, 20639, 1010, 3272, 2006, 9486, 1998, 7082, 2015, 2073, 2027, 2024, 17804, 1011, 2829, 1012, 28487, 10663, 2044, 2070, 2706, 2000, 2681, 26916, 1011, 2829, 2695, 1011, 20187, 6097, 16308, 2008, 2064, 2202, 1037, 2095, 2030, 2936, 2000, 12985, 1012, 3013, 17191, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 4556, 10482, 3401, 3560, 26572, 20028, 28487, 5622, 8661, 2933, 2271, 15536, 3600, 3511, 1005, 1055, 2358, 4360, 2063, 5622, 8661, 2933, 2271, 9535, 12259, 28487, 5622, 8661, 2933, 2271, 23760, 13181, 17926, 28487, 5622, 8661, 2933, 2271, 3024, 2011, 5622, 8661, 2933, 2271, 23760, 13181, 17926, 28487, 2156, 2062, 4871, 1997, 5622, 8661, 2933, 2271, 1012, 8700, 5622, 8661, 2933, 2271, 1996, 2677, 2003, 2411, 1996, 2069, 5360, 2181, 1012, 8700, 5622, 8661, 2933, 2271, 2411, 7336, 1996, 2503, 1997, 1996, 6029, 1998, 1996, 3903, 1997, 1996, 4416, 1010, 2021, 1996, 16031, 2015, 1998, 2970, 2089, 2036, 2022, 2920, 1012, 1996, 2087, 2691, 7060, 2024, 1024, 3255, 3238, 2317, 21295, 1999, 1037, 19959, 2030, 20863, 1011, 2066, 5418, 9145, 1998, 14516, 14173, 2015, 1998, 17359, 17119, 2015, 1006, 9413, 20049, 3726, 5622, 8661, 2933, 2271, 1007, 28105, 2417, 2791, 1998, 28241, 1997, 1996, 16031, 2015, 1006, 4078, 16211, 18900, 3512, 18353, 5856, 5737, 7315, 1007, 2334, 5084, 21733, 1997, 1996, 16031, 2015, 5516, 2000, 25933, 27887, 2213, 8110, 2015, 1012, 8700, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 9413, 20049, 3726, 8700, 5622, 8661, 2933, 2271, 9413, 20049, 3726, 8700, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 24728, 22144, 2140, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 2089, 7461, 6845, 2401, 2350, 2050, 1010, 6845, 2401, 3576, 2050, 1998, 12436, 24965, 17174, 4183, 2271, 1012, 8312, 2950, 1024, 3255, 3238, 2317, 21295, 1999, 1037, 19959, 2030, 20863, 1011, 2066, 5418, 9145, 1998, 14516, 14173, 2015, 1998, 17359, 17119, 2015, 1006, 9413, 20049, 3726, 5622, 8661, 2933, 2271, 1007, 11228, 4892, 1010, 4525, 1999, 4748, 21471, 2015, 1010, 24501, 2953, 16790, 1997, 6845, 2401, 3576, 2050, 1998, 17174, 18400, 26261, 27109, 9145, 4078, 16211, 18900, 3512, 12436, 11528, 13706, 1010, 10723, 23198, 1998, 4786, 1037, 14163, 17413, 12436, 24965, 11889, 1012, 1996, 23300, 12436, 20876, 2089, 19501, 4089, 2006, 3967, 17702, 2007, 24728, 22144, 2140, 5622, 8661, 8040, 3917, 2891, 2271, 1010, 2019, 20187, 3096, 8761, 2008, 2087, 4141, 13531, 2308, 2058, 2753, 2086, 1997, 2287, 1012, 7279, 9463, 5622, 8661, 2933, 2271, 7279, 9463, 5622, 8661, 2933, 2271, 2788, 7534, 2007, 4556, 6643, 14289, 4244, 1999, 1037, 3614, 2105, 1996, 1043, 5802, 2015, 1012, 2317, 21295, 1998, 9413, 20049, 3726, 5622, 8661, 2933, 2271, 2089, 5258, 2021, 2024, 2625, 2691, 1012, 4871, 1997, 8991, 18400, 5622, 8661, 2933, 2271, 2060, 14163, 13186, 2389, 4573, 9413, 20049, 3726, 5622, 8661, 2933, 2271, 13191, 2135, 13531, 1996, 18749, 20026, 2389, 23340, 1010, 16544, 1010, 6327, 4540, 5033, 1010, 1051, 2229, 7361, 3270, 12349, 1010, 2474, 18143, 2595, 1010, 24176, 1998, 2019, 2271, 1012, 5622, 8661, 2933, 7361, 11733, 6935, 5622, 8661, 2933, 7361, 11733, 6935, 7534, 2004, 4714, 2417, 6714, 2100, 1042, 14511, 21412, 6643, 14289, 4244, 1998, 8402, 5744, 2752, 2006, 1996, 21065, 2030, 2625, 2411, 1010, 6974, 2006, 1996, 2303, 1012, 6524, 1010, 1038, 9863, 7999, 5158, 1999, 1996, 22520, 1012, 6215, 1997, 1996, 2606, 1042, 14511, 20921, 5260, 2000, 8642, 13852, 13864, 17253, 2011, 20288, 1005, 9479, 13606, 1005, 1012, 19124, 10882, 12618, 7741, 2632, 17635, 7405, 2003, 1037, 2433, 1997, 5622, 8661, 2933, 7361, 11733, 6935, 2008, 13531, 1996, 15099, 21065, 1010, 6130, 1998, 8407, 1012, 18404, 11880, 9648, 1997, 22953, 2278, 4160, 2089, 2022, 1037, 8349, 1997, 5622, 8661, 2933, 2271, 2302, 21733, 2030, 25169, 1012, 2752, 1997, 11228, 4892, 2302, 2606, 3254, 3711, 1010, 2649, 2004, 1005, 2066, 24629, 1999, 1996, 4586, 1005, 1012, 5622, 8661, 2933, 2271, 12473, 1996, 21065, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 10882, 12618, 7741, 2632, 17635, 7405, 10882, 12618, 7741, 2632, 17635, 7405, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 13774, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 13531, 2028, 2030, 2062, 10063, 1010, 2823, 2302, 5994, 1996, 3096, 3302, 1012, 2009, 2003, 2170, 3174, 1011, 13774, 1040, 27268, 18981, 10536, 2065, 2035, 10063, 2024, 19470, 1998, 7880, 2842, 2003, 5360, 1012, 5622, 8661, 2933, 2271, 4857, 2015, 1996, 13774, 5127, 1010, 2029, 2089, 2468, 14100, 2094, 1998, 5526, 2094, 1012, 1996, 13774, 2089, 2601, 2368, 1010, 4317, 2368, 2030, 6336, 2125, 1996, 13774, 2793, 1006, 2006, 17994, 4747, 20960, 1007, 1012, 2823, 1996, 3013, 25128, 2003, 3908, 1998, 3596, 1037, 11228, 1006, 13866, 7301, 5856, 2819, 1007, 1012, 1996, 10063, 2089, 8328, 2030, 2644, 3652, 10462, 1010, 1998, 2027, 2089, 6524, 1010, 3294, 10436, 1006, 2019, 16585, 20881, 1007, 1012, 5622, 8661, 2933, 2271, 1997, 10063, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 1997, 13774, 1075, 2852, 6887, 11113, 14428, 2571, 2278, 1516, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 2156, 2062, 4871, 1997, 5622, 8661, 2933, 2271, 1997, 1996, 10063, 1012, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 5577, 5665, 1011, 4225, 9242, 1010, 26916, 2829, 6017, 2006, 1996, 2227, 1998, 3300, 2030, 8260, 1998, 10726, 2302, 2019, 20187, 4403, 1012, 2009, 2003, 1037, 2433, 1997, 3734, 4315, 9067, 6097, 7934, 23760, 8197, 21693, 19304, 1012, 2009, 2064, 2022, 19157, 2011, 3103, 7524, 1010, 2021, 2009, 2064, 2036, 13368, 1999, 3103, 1011, 5123, 4573, 2107, 2004, 1996, 2849, 23270, 2015, 1012, 2009, 2038, 28105, 1010, 2128, 4588, 9869, 1998, 28105, 7060, 1012, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 2003, 2714, 2000, 9413, 26688, 2863, 1040, 7274, 2818, 21716, 22167, 2566, 12693, 2015, 1998, 2453, 2022, 1996, 2168, 4295, 1012, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 2089, 6524, 7461, 1996, 2970, 1010, 4525, 1999, 1037, 8983, 2100, 2601, 28815, 3370, 2006, 3356, 1998, 2896, 2970, 1012, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 1997, 2227, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 1997, 2227, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 1997, 2896, 2067, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 1997, 2896, 2067, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 1997, 2896, 5423, 5622, 8661, 2933, 2271, 28815, 2891, 2271, 1997, 2896, 5423, 5622, 8661, 9314, 4319, 17259, 5622, 8661, 9314, 4319, 17259, 5218, 2000, 1037, 5622, 8661, 2933, 2271, 1011, 2066, 23438, 3303, 2011, 20992, 1012, 2004, 24335, 13876, 9626, 4588, 2030, 2009, 11714, 1025, 5061, 1010, 2829, 2030, 6379, 1025, 4257, 1010, 3621, 8040, 20766, 13864, 2087, 2411, 13368, 2006, 1996, 8260, 1012, 1996, 8700, 14163, 13186, 2050, 1006, 8700, 5622, 8661, 9314, 4668, 1007, 1998, 2060, 4573, 2024, 2036, 2823, 5360, 1012, 2116, 5850, 2064, 6524, 3426, 5622, 8661, 9314, 28448, 1012, 1996, 2087, 2691, 2024, 1024, 2751, 18479, 18037, 2818, 10626, 2080, 12519, 2063, 14408, 7361, 15928, 1012, 21864, 19105, 1998, 16215, 2401, 5831, 3207, 4487, 5397, 14606, 3426, 1037, 7760, 6132, 13043, 5622, 8661, 9314, 4319, 17259, 1012, 2054, 2024, 1996, 12763, 1997, 5622, 8661, 2933, 2271, 1029, 23760, 13181, 17926, 5622, 8661, 2933, 2271, 2089, 13014, 5490, 6692, 27711, 3526, 2482, 21081, 2863, 1012, 2174, 1010, 6524, 1010, 2146, 24911, 9413, 20049, 3726, 5622, 8661, 2933, 2271, 2064, 2765, 1999, 2995, 5490, 6692, 27711, 3526, 2482, 21081, 2863, 1010, 2087, 2411, 1999, 1996, 2677, 1006, 8700, 4456, 1007, 2030, 2006, 1996, 24728, 22144, 1006, 24728, 22144, 2140, 4456, 1007, 2030, 19085, 1006, 7279, 9463, 4456, 1007, 1012, 2023, 2323, 2022, 6878, 2065, 2045, 2003, 2019, 4372, 8017, 4726, 7293, 9307, 2030, 2019, 17359, 17119, 2007, 4317, 6675, 7926, 1999, 2122, 4573, 1012, 4456, 2003, 2062, 2691, 1999, 5610, 2869, 1010, 2216, 2007, 1037, 2381, 1997, 4456, 1999, 14163, 13186, 2389, 4573, 1010, 1998, 1999, 2216, 2040, 4287, 12581, 3734, 1998, 2006, 3597, 16505, 2529, 6643, 8197, 7174, 2863, 23350, 1012, 4456, 2013, 2060, 3596, 1997, 5622, 8661, 2933, 2271, 2003, 4678, 1012, 2129, 2003, 5622, 8661, 2933, 2271, 11441, 1029, 1999, 2087, 3572, 1010, 5622, 8661, 2933, 2271, 2003, 11441, 2011, 14158, 2049, 6612, 2838, 1012, 1037, 16012, 18075, 2003, 2411, 6749, 2000, 12210, 2030, 2191, 1996, 11616, 1998, 2000, 2298, 2005, 4456, 1012, 1996, 2010, 14399, 8988, 10091, 5751, 2024, 1997, 1037, 5622, 8661, 9314, 8153, 4668, 12473, 1996, 4958, 18688, 15630, 1012, 5171, 2838, 2421, 1024, 12052, 2135, 4317, 6675, 4958, 18688, 15630, 2139, 6914, 25284, 3096, 4442, 5622, 4226, 7011, 7542, 2139, 6914, 16754, 1997, 1996, 15191, 6741, 1997, 1996, 4958, 18688, 15630, 2316, 1997, 20187, 4442, 2074, 4218, 1996, 4958, 18688, 15630, 11463, 7088, 2078, 1006, 28815, 1007, 4218, 1996, 4958, 18688, 15630, 3622, 10047, 23041, 11253, 7630, 16610, 13013, 21101, 2075, 2089, 7487, 10042, 1997, 10047, 23041, 8649, 4135, 8569, 24412, 2012, 1996, 2918, 1997, 1996, 4958, 18688, 15630, 1012, 8983, 5852, 2089, 2022, 6749, 2005, 5022, 2007, 8700, 5622, 8661, 2933, 2271, 12473, 1996, 16031, 2015, 1998, 2040, 2031, 25933, 27887, 2213, 8110, 2015, 1010, 2000, 14358, 2005, 3967, 2035, 24395, 2000, 16215, 18994, 2545, 2389, 1006, 1037, 21442, 10841, 14482, 7328, 1007, 1012, 2054, 2003, 1996, 3949, 2005, 5622, 8661, 2933, 2271, 1029, 3949, 2003, 2025, 2467, 4072, 1012, 2334, 13441, 2005, 1996, 25353, 27718, 9626, 4588, 3013, 17191, 2030, 14163, 13186, 2389, 4295, 2024, 1024, 16834, 25665, 26261, 29514, 25665, 10250, 16567, 9496, 2078, 25456, 1010, 11937, 26775, 10893, 7606, 1051, 18447, 3672, 1998, 14255, 4168, 26775, 10893, 7606, 6949, 25665, 2128, 25690, 9821, 26721, 4244, 19301, 26261, 22943, 13341, 2015, 22575, 3949, 2005, 6923, 5622, 8661, 2933, 2271, 2030, 5729, 2334, 4295, 2411, 2950, 1037, 1015, 2000, 1017, 1011, 3204, 2607, 1997, 8700, 3653, 2094, 8977, 5643, 1010, 2096, 25819, 2178, 4005, 2013, 1996, 2206, 2862, 1024, 9353, 4183, 13465, 2378, 18479, 18037, 2818, 10626, 2080, 12519, 2063, 2777, 12326, 2890, 18684, 2618, 17207, 25457, 7361, 11467, 2026, 3597, 8458, 16515, 13806, 9587, 7959, 3775, 2140, 6302, 20900, 1999, 3572, 1997, 8700, 5622, 8661, 2933, 2271, 12473, 1996, 16031, 2015, 2007, 3967, 2035, 24395, 2000, 8714, 1010, 1996, 5622, 8661, 2933, 2271, 2089, 10663, 2006, 6419, 1996, 8110, 2015, 2007, 12490, 3430, 1012, 2065, 1996, 5622, 8661, 2933, 2271, 2003, 2025, 2349, 2000, 8714, 2035, 24395, 1010, 9268, 25933, 27887, 2213, 8110, 2015, 2003, 2200, 9832, 2000, 2765, 1999, 1037, 9526, 1012, 2019, 8586, 27364, 2389, 3112, 2003, 2988, 2013, 2146, 5352, 1997, 8700, 24479, 1998, 8700, 3424, 11263, 13807, 2140, 6074, 1012, 5622, 8661, 2933, 7361, 11733, 6935, 2003, 2988, 2000, 5335, 2007, 14255, 8649, 27606, 15975, 1012, 2054, 2003, 1996, 17680, 2005, 5622, 8661, 2933, 2271, 1029, 3013, 17191, 5622, 8661, 2933, 2271, 12102, 2000, 3154, 2306, 1037, 3232, 1997, 2086, 1999, 2087, 2111, 1010, 2021, 14163, 13186, 2389, 5622, 8661, 2933, 2271, 2003, 2062, 3497, 2000, 29486, 2005, 1037, 5476, 2030, 2936, 1012, 17630, 7233, 2003, 21446, 1010, 1998, 5622, 8661, 2933, 2271, 2089, 28667, 3126, 2012, 1037, 2101, 3058, 1012, 11228, 4892, 2003, 4568, 1010, 2164, 13852, 2075, 1997, 1996, 21065, 1012, 5622, 8661, 9314, 4319, 28448, 3154, 2039, 3254, 2043, 1996, 3625, 14667, 2003, 9633, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(batch_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-2816b7489215>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "tokenizer(text[0][0].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-f1e124328469>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2281\u001b[0m             )\n\u001b[0;32m   2282\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2283\u001b[1;33m             \u001b[1;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2284\u001b[0m             \u001b[1;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m         )\n",
      "\u001b[1;31mAssertionError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "tokenizer(text[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 16770, 1024, 1013, 1013, 4372, 1012, 16948, 1012, 8917, 1013, 15536, 3211, 1013, 5622, 8661, 1035, 2933, 2271, 5622, 8661, 2933, 2271, 1006, 6948, 1007, 2003, 1037, 11888, 20187, 1998, 11311, 1011, 19872, 4295, 2008, 13531, 1996, 3096, 1010, 10063, 1010, 2606, 1010, 1998, 14163, 27199, 24972, 1012, 1031, 1015, 1033, 2009, 2003, 7356, 2011, 26572, 20028, 1010, 4257, 1011, 9370, 1010, 10482, 3401, 3560, 6643, 14289, 4244, 1998, 28487, 2007, 15241, 2075, 1010, 2128, 4588, 8898, 1010, 2986, 2317, 4094, 1006, 15536, 3600, 3511, 1005, 1055, 2358, 4360, 2063, 1007, 1010, 4141, 12473, 12759, 2398, 1010, 23951, 11137, 12150, 1998, 27323, 1010, 8260, 1010, 15099, 2896, 3456, 1998, 8700, 14163, 13186, 2050, 1012, 1031, 1016, 1033, 2348, 2045, 2003, 1037, 5041, 6612, 2846, 1997, 6948, 24491, 2015, 1010, 1996, 3096, 1998, 8700, 17790, 3961, 2004, 1996, 2350, 4573, 1997, 6624, 1012, 1031, 1017, 1033, 1996, 3426, 2003, 4242, 1010, 2021, 2009, 2003, 2245, 2000, 2022, 1996, 2765, 1997, 2019, 8285, 5714, 23041, 2063, 2832, 2007, 2019, 4242, 3988, 9495, 1012, 2045, 2003, 2053, 9526, 1010, 2021, 2116, 2367, 20992, 1998, 8853, 2031, 2042, 2109, 1999, 4073, 2000, 2491, 1996, 8030, 1012, 1996, 2744, 5622, 8661, 9314, 4668, 1006, 5622, 8661, 9314, 17259, 2030, 5622, 8661, 9314, 4649, 3258, 1007, 5218, 2000, 1037, 4649, 3258, 1997, 2714, 2030, 7235, 2010, 14399, 8988, 12898, 12863, 1998, 6612, 3311, 2000, 5622, 8661, 2933, 2271, 1006, 1045, 1012, 1041, 1012, 1010, 2019, 2181, 2029, 12950, 5622, 8661, 2933, 2271, 1010, 2119, 2000, 1996, 6248, 3239, 1998, 2104, 1037, 24635, 1007, 1012, 1031, 1018, 1033, 1031, 1019, 1033, 2823, 11394, 4475, 2030, 3056, 20992, 2064, 3426, 1037, 5622, 8661, 9314, 4668, 1012, 1031, 1018, 1033, 2027, 2064, 2036, 5258, 1999, 2523, 2007, 22160, 2102, 6431, 3677, 4295, 1012, 1031, 1018, 1033, 1031, 1020, 1033, 1024, 24398, 8417, 1015, 5579, 1015, 1012, 1015, 2609, 1015, 1012, 1016, 5418, 1015, 1012, 1017, 17702, 8715, 2015, 1016, 5751, 1998, 8030, 1016, 1012, 1015, 3096, 1016, 1012, 1016, 14163, 27199, 24972, 1017, 5320, 1018, 26835, 19009, 1019, 11616, 1019, 1012, 1015, 3096, 1019, 1012, 1016, 2677, 1019, 1012, 1017, 11658, 11616, 1019, 1012, 1018, 2010, 14399, 8988, 6779, 1020, 3949, 1020, 1012, 1015, 3096, 1020, 1012, 1016, 2677, 1021, 4013, 26745, 6190, 1022, 4958, 5178, 4328, 6779, 1023, 2381, 2184, 2470, 2340, 3964, 2260, 7604, 2410, 6327, 6971, 5579, 5622, 8661, 2933, 2271, 22520, 2024, 2061, 2170, 2138, 1997, 2037, 1000, 5622, 8661, 1011, 2066, 1000, 3311, 1031, 1021, 1033, 1998, 2064, 2022, 6219, 2011, 1996, 2609, 2027, 9125, 1010, 2030, 2011, 2037, 19476, 1012, 2609, 5622, 8661, 2933, 2271, 2089, 2022, 20427, 2004, 12473, 14163, 13186, 2389, 2030, 3013, 17191, 9972, 1012, 3013, 17191, 3596, 2024, 2216, 12473, 1996, 3096, 1010, 21065, 1010, 1998, 10063, 1012, 1031, 1022, 1033, 1031, 1023, 1033, 1031, 2184, 1033, 14163, 13186, 2389, 3596, 2024, 2216, 12473, 1996, 14834, 1997, 1996, 3806, 13181, 18447, 19126, 12859, 1006, 2677, 1010, 6887, 5649, 26807, 1010, 9686, 7361, 3270, 12349, 1010, 4308, 1010, 2019, 2271, 1007, 1010, 2474, 18143, 2595, 1010, 1998, 2060, 14163, 13186, 102], [101, 16770, 1024, 1013, 1013, 7479, 1012, 17237, 1012, 2866, 1013, 3785, 1013, 5622, 8661, 1011, 2933, 2271, 1013, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 2003, 1037, 23438, 2008, 2064, 7461, 2367, 3033, 1997, 2115, 2303, 1010, 2164, 1996, 2503, 1997, 2115, 2677, 1012, 2156, 1037, 14246, 2065, 2017, 2228, 2017, 2453, 2031, 2009, 1012, 2512, 1011, 13661, 6040, 1024, 2156, 1037, 14246, 2065, 2017, 2031, 1024, 12906, 1997, 12538, 1010, 2992, 1010, 6379, 1011, 2417, 1038, 10994, 8376, 2006, 2115, 2608, 1010, 3456, 2030, 2303, 1006, 2017, 2089, 2156, 2986, 2317, 3210, 2006, 1996, 1038, 10994, 8376, 1007, 2317, 13864, 2006, 2115, 16031, 2015, 1010, 4416, 2030, 1996, 19008, 1997, 2115, 6029, 5255, 1998, 22748, 1999, 2115, 2677, 1010, 2926, 2043, 2017, 4521, 2030, 4392, 13852, 13864, 6037, 2006, 2115, 21065, 14699, 2417, 13864, 2006, 2115, 24728, 22144, 5931, 1010, 4857, 5582, 10063, 2007, 25880, 2006, 3614, 1011, 5044, 6379, 2030, 2317, 13864, 2006, 2115, 19085, 2122, 2024, 8030, 1997, 5622, 8661, 2933, 2271, 1012, 2017, 2089, 2069, 2031, 1015, 1997, 2122, 8030, 1012, 5622, 8661, 2933, 2271, 2006, 2115, 3096, 2064, 2022, 2200, 2009, 11714, 1010, 2021, 2025, 2467, 1012, 2592, 1024, 21887, 23350, 10651, 1024, 2129, 2000, 3967, 1037, 14246, 2009, 1005, 1055, 2145, 2590, 2000, 2131, 2393, 2013, 1037, 14246, 2065, 2017, 2342, 2009, 1012, 2000, 3967, 2115, 14246, 5970, 1024, 3942, 2037, 4037, 2224, 1996, 17237, 10439, 2655, 2068, 2424, 2041, 2055, 2478, 1996, 17237, 2076, 21887, 23350, 5622, 8661, 2933, 2271, 2006, 1996, 2503, 1997, 1996, 7223, 5622, 8661, 2933, 2271, 2411, 3544, 2006, 1996, 2503, 1997, 2115, 7223, 2317, 13864, 1997, 5622, 8661, 2933, 2271, 1999, 1996, 2677, 2317, 13864, 1999, 2115, 2677, 2089, 2022, 5622, 8661, 2933, 2271, 2065, 2017, 1005, 2128, 2025, 2469, 2009, 1005, 1055, 5622, 8661, 2933, 2271, 13441, 2013, 1037, 14246, 5622, 8661, 2933, 2271, 2006, 2115, 3096, 2788, 4152, 2488, 2006, 2049, 2219, 1999, 2055, 1023, 2000, 2324, 2706, 1012, 6949, 2015, 1998, 1051, 18447, 8163, 2013, 1037, 14246, 2064, 2393, 2491, 1996, 23438, 1998, 7496, 2009, 8450, 1012, 2065, 6949, 2015, 1998, 1051, 18447, 8163, 2079, 2025, 2147, 2030, 2017, 2031, 5729, 5622, 8661, 2933, 2271, 1010, 26261, 22943, 17596, 2030, 3949, 2007, 1037, 2569, 2785, 1997, 2422, 1006, 2422, 7242, 1007, 2064, 2393, 1012, 5622, 8661, 2933, 2271, 1999, 2115, 2677, 2064, 2197, 2005, 2195, 2086, 1012, 2677, 28556, 2229, 1998, 12509, 2015, 2013, 1037, 14246, 2064, 2393, 7496, 8030, 2066, 5255, 2030, 14699, 16031, 2015, 1012, 2017, 3685, 4608, 5622, 8661, 2933, 2271, 1998, 2009, 2515, 2025, 2788, 2272, 2067, 2320, 2009, 1005, 1055, 5985, 2039, 1012, 2005, 2490, 1998, 2592, 1010, 2156, 2866, 5622, 8661, 2933, 2271, 1012, 2129, 2000, 15804, 5622, 8661, 2933, 2271, 2012, 2188, 2065, 2017, 2031, 5622, 8661, 2933, 2271, 2006, 2115, 3096, 1024, 9378, 2007, 5810, 4010, 2300, 1516, 4468, 7815, 2015, 1998, 2303, 9378, 2229, 9378, 2115, 2606, 2058, 1037, 7752, 2030, 7198, 2061, 1996, 25850, 24667, 2515, 2025, 2272, 2046, 3967, 2007, 1996, 2717, 1997, 2115, 3096, 2224, 2019, 7861, 14511, 11638, 1006, 11052, 9496, 102], [101, 16770, 1024, 1013, 1013, 7479, 1012, 14415, 20464, 5498, 2278, 1012, 8917, 1013, 7870, 1011, 3785, 1013, 5622, 8661, 1011, 2933, 2271, 1013, 8030, 1011, 5320, 1013, 25353, 2278, 1011, 18540, 22203, 24434, 2620, 1001, 1024, 1066, 1024, 3793, 1027, 5622, 8661, 1003, 2322, 24759, 20849, 1003, 2322, 1006, 4682, 1003, 14134, 5283, 2078, 1003, 2322, 13068, 1010, 2008, 1003, 2322, 24844, 18349, 2361, 1003, 2322, 7840, 1003, 27074, 22507, 2389, 1003, 2322, 28075, 2015, 1012, 19184, 6302, 1997, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 2330, 3769, 1011, 2039, 13764, 8649, 3482, 8458, 11439, 1997, 5622, 8661, 2933, 2271, 5622, 8661, 2933, 2271, 1997, 10063, 2330, 3769, 1011, 2039, 13764, 8649, 3482, 6525, 2140, 5622, 8661, 2933, 2271, 22520, 3426, 19959, 2317, 13864, 1999, 1996, 2677, 1012, 8700, 5622, 8661, 2933, 2271, 2330, 3769, 1011, 2039, 13764, 8649, 3482, 5622, 8661, 2933, 2271, 1006, 4682, 1011, 28919, 2377, 1011, 16371, 2015, 1007, 2003, 1037, 4650, 2008, 2064, 3426, 18348, 1998, 17373, 1999, 1996, 3096, 1010, 2606, 1010, 10063, 1998, 14163, 27199, 24972, 1012, 2006, 1996, 3096, 1010, 5622, 8661, 2933, 2271, 2788, 3544, 2004, 16405, 14536, 13602, 1010, 2009, 11714, 1010, 4257, 18548, 2008, 4503, 2058, 2195, 3134, 1012, 1999, 1996, 2677, 1010, 12436, 20876, 1998, 2060, 2752, 3139, 2011, 1037, 14163, 27199, 10804, 1010, 5622, 8661, 2933, 2271, 3596, 19959, 2317, 13864, 1010, 2823, 2007, 9145, 14699, 2015, 1012, 2087, 2111, 2064, 6133, 5171, 1010, 10256, 3572, 1997, 5622, 8661, 2933, 2271, 2012, 2188, 1010, 2302, 2966, 2729, 1012, 2065, 1996, 4650, 5320, 3255, 2030, 3278, 2009, 8450, 1010, 2017, 2089, 2342, 20422, 5850, 1012, 5622, 8661, 2933, 2271, 3475, 1005, 1056, 9530, 15900, 6313, 1012, 3688, 1004, 2578, 2338, 1024, 14415, 9349, 2155, 2740, 2338, 1010, 4833, 3179, 2265, 2062, 3688, 2013, 14415, 9349, 8030, 1996, 5751, 1998, 8030, 1997, 5622, 8661, 2933, 2271, 8137, 5834, 2006, 1996, 2752, 5360, 1012, 5171, 5751, 1998, 8030, 2024, 1024, 16405, 14536, 13602, 1010, 4257, 18548, 1010, 2087, 2411, 2006, 1996, 5110, 16148, 1010, 7223, 2030, 10792, 1010, 1998, 2823, 1996, 8991, 18400, 2015, 2009, 8450, 1038, 9863, 2545, 2008, 3338, 2000, 2433, 8040, 7875, 2015, 2030, 19116, 2015, 19959, 2317, 13864, 1999, 1996, 2677, 2030, 2006, 1996, 2970, 2030, 4416, 9145, 14699, 2015, 1999, 1996, 2677, 2030, 12436, 20876, 2606, 3279, 2689, 1999, 21065, 3609, 13774, 4053, 2030, 3279, 2043, 2000, 2156, 1037, 3460, 2156, 2115, 3460, 2065, 4714, 18548, 2030, 1037, 23438, 1011, 2066, 4650, 3544, 2006, 2115, 3096, 2005, 2053, 6835, 3114, 1010, 2107, 2004, 1037, 2124, 27395, 4668, 2030, 3967, 2007, 9947, 7768, 1012, 2036, 2156, 2115, 3460, 2065, 2017, 3325, 2151, 5751, 2030, 8030, 3378, 2007, 5622, 8661, 2933, 2271, 1997, 1996, 2677, 1010, 8991, 18400, 2015, 1010, 21065, 2030, 10063, 1012, 2009, 1005, 1055, 2190, 2000, 2131, 1037, 25732, 1998, 8321, 11616, 2138, 1037, 2193, 1997, 3096, 1998, 14163, 13186, 2389, 3785, 2064, 3426, 22520, 1998, 17964, 1012, 5227, 2019, 6098, 2012, 14415, 9349, 5320, 5622, 8661, 2933, 2271, 5158, 2043, 2115, 11311, 2291, 4491, 4442, 1997, 1996, 3096, 2030, 14163, 27199, 24972, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 files belonging to 2 classes.\n",
      "Using 11 files for training.\n",
      "Found 13 files belonging to 2 classes.\n",
      "Using 11 files for training.\n",
      "2020-12-02 20:17:43,825 — dermclass_models2.preprocessing — INFO —_load_data_tf:130 — Successfully loaded train and validation datasets \n",
      "2020-12-02 20:17:43,833 — dermclass_models2.preprocessing — INFO —_split_train_test_tf:152 — Number of train batches: 6        Number of validation batches: 3        Number of test batches: 3\n"
     ]
    }
   ],
   "source": [
    "train_dataset, _, _ = pp.load_data(use_generators=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (512,), attention_mask: (512,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "###################\n",
    "###################\n",
    "# FREEZE FIRST LAYERS\n",
    "###################\n",
    "###################\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline prepare model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(df['target'].unique()))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataset:\n",
    "    batch = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       " array([b\"https://www.nhs.uk/conditions/lichen-planus/\\r\\n\\r\\nLichen planus\\r\\nLichen planus is a rash that can affect different parts of your body, including the inside of your mouth. See a GP if you think you might have it.\\r\\n\\r\\nNon-urgent advice:See a GP if you have:\\r\\nclusters of shiny, raised, purple-red blotches on your arms, legs or body (you may see fine white lines on the blotches)\\r\\nwhite patches on your gums, tongue or the insides of your cheeks\\r\\nburning and stinging in your mouth, especially when you eat or drink\\r\\nbald patches appearing on your scalp\\r\\nsore red patches on your vulva\\r\\nrough, thinning nails with grooves on\\r\\nring-shaped purple or white patches on your penis\\r\\nThese are symptoms of lichen planus. You may only have 1 of these symptoms.\\r\\n\\r\\nLichen planus on your skin can be very itchy, but not always.\\r\\n\\r\\nInformation:\\r\\nCoronavirus update: how to contact a GP\\r\\nIt's still important to get help from a GP if you need it. To contact your GP surgery:\\r\\n\\r\\nvisit their website\\r\\nuse the NHS App\\r\\ncall them\\r\\nFind out about using the NHS during coronavirus\\r\\n\\r\\nLichen planus on the inside of the wrist\\r\\nLichen planus often appears on the inside of your wrist\\r\\nWhite patches of lichen planus in the mouth\\r\\nWhite patches in your mouth may be lichen planus\\r\\nIf you're not sure it's lichen planus\\r\\nTreatments from a GP\\r\\nLichen planus on your skin usually gets better on its own in about 9 to 18 months.\\r\\n\\r\\nCreams and ointments from a GP can help control the rash and ease itching.\\r\\n\\r\\nIf creams and ointments do not work or you have severe lichen planus, steroid tablets or treatment with a special kind of light (light therapy) can help.\\r\\n\\r\\nLichen planus in your mouth can last for several years. Mouthwashes and sprays from a GP can help ease symptoms like burning or sore gums.\\r\\n\\r\\nYou cannot catch lichen planus and it does not usually come back once it's cleared up.\\r\\n\\r\\nFor support and information, see UK Lichen Planus.\\r\\n\\r\\nHow to relieve lichen planus at home\\r\\nIf you have lichen planus on your skin:\\r\\nwash with plain warm water \\xe2\\x80\\x93 avoid soaps and body washes\\r\\nwash your hair over a sink or bath so the shampoo does not come into contact with the rest of your skin\\r\\nuse an emollient (moisturising treatment for the skin) on the rash\\r\\nIf the lichen planus is on your genitals:\\r\\nhold a bag of frozen peas wrapped in a clean tea towel against the affected bits to ease itching and swelling\\r\\navoid wearing tights or close-fitting clothes\\r\\nIf you have it in your mouth:\\r\\nbrush your teeth carefully twice a day to keep your gums healthy\\r\\navoid salty, spicy or acidic foods if they make your mouth sore\\r\\navoid alcohol and mouthwashes that contain it\",\n",
       "        b\"https://www.nhs.uk/conditions/psoriasis/symptoms/\\r\\n\\r\\nMain symptoms of psoriasis\\r\\nPsoriasis typically causes patches of skin that are dry, red and covered in silver scales. Some people find their psoriasis causes itching or soreness.\\r\\n\\r\\nThere are several different types of psoriasis. Many people have only 1 form at a time, although 2 different types can occur together. One form may change into another or become more severe.\\r\\n\\r\\nMost cases of psoriasis go through cycles, causing problems for a few weeks or months before easing or stopping.\\r\\n\\r\\nYou should see a GP if you think you may have psoriasis.\\r\\n\\r\\nCommon types of psoriasis\\r\\nPlaque psoriasis (psoriasis vulgaris)\\r\\nPlaque psoriasis rash.\\r\\nThis is the most common form, accounting for about 80 to 90% of cases.\\r\\n\\r\\nIts symptoms are dry red skin lesions, known as plaques, covered in silver scales.\\r\\n\\r\\nThey normally appear on your elbows, knees, scalp and lower back, but can appear anywhere on your body.\\r\\n\\r\\nThe plaques can be itchy or sore, or both. In severe cases, the skin around your joints may crack and bleed. \\r\\n\\r\\nScalp psoriasis\\r\\nThis can occur on parts of your scalp or on the whole scalp. It causes red patches of skin covered in thick, silvery-white scales.\\r\\n\\r\\nSome people find scalp psoriasis extremely itchy, while others have no discomfort.\\r\\n\\r\\nIn extreme cases, it can cause hair loss, although this is usually only temporary.\\r\\n\\r\\nNail psoriasis\\r\\nIn about half of all people with psoriasis, the condition affects the nails.\\r\\n\\r\\nPsoriasis can cause your nails to develop tiny dents or pits, become discoloured or grow abnormally.\\r\\n\\r\\nNails can often become loose and separate from the nail bed. In severe cases, nails may crumble.\\r\\n\\r\\nGuttate psoriasis\\r\\nGuttate psoriasis causes small (less than 1cm) drop-shaped sores on your chest, arms, legs and scalp.\\r\\n\\r\\nThere's a good chance guttate psoriasis will disappear completely after a few weeks, but some people go on to develop plaque psoriasis.\\r\\n\\r\\nThis type of psoriasis sometimes occurs after a streptococcal throat infection and is more common among children and teenagers.\\r\\n\\r\\nInverse (flexural) psoriasis\\r\\nThis affects folds or creases in your skin, such as the armpits, groin, between the buttocks and under the breasts.\\r\\n\\r\\nIt can cause large, smooth red patches in some or all these areas.\\r\\n\\r\\nInverse psoriasis is made worse by friction and sweating, so it can be particularly uncomfortable in hot weather.\\r\\n\\r\\nLess common types of psoriasis\\r\\nPustular psoriasis\\r\\nPustular psoriasis is a rarer type of psoriasis that causes pus-filled blisters (pustules) to appear on your skin.\\r\\n\\r\\nDifferent types of pustular psoriasis affect different parts of the body.\\r\\n\\r\\nGeneralised pustular psoriasis or von Zumbusch psoriasis\\r\\nThis causes pustules that develop very quickly on a wide area of skin. The pus consists of white blood cells and is not a sign of infection.\\r\\n\\r\\nThe pustules may reappear every few days or weeks in cycles. During the start of these cycles, von Zumbusch psoriasis can cause fever, chills, weight loss and fatigue.\\r\\n\\r\\nPalmoplantar pustulosis\\r\\nThis causes pustules to appear on the palms of your hands and the soles of your feet.\\r\\n\\r\\nThe pustules gradually develop into circular, brown, scaly spots that then peel off.\\r\\n\\r\\nPustules may reappear every few days or weeks.\\r\\n\\r\\nAcropustulosis\\r\\nThis causes pustules to appear on your fingers and toes.\\r\\n\\r\\nThe pustules then burst, leaving bright red areas that may ooze or become scaly. These may lead to painful nail deformities.\\r\\n\\r\\nErythrodermic psoriasis\\r\\nErythrodermic psoriasis is a rare form of psoriasis that affects nearly all the skin on the body. This can cause intense itching or burning.\\r\\n\\r\\nErythrodermic psoriasis can cause your body to lose proteins and fluid, leading to further problems such as infection, dehydration, heart failure, hypothermia and malnutrition.\"],\n",
       "       dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1])>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       "  array([  101, 16770,  1024,  1013,  1013,  4372,  1012, 16948,  1012,\n",
       "          8917,  1013, 15536,  3211,  1013,  5622,  8661,  1035,  2933,\n",
       "          2271,  5622,  8661,  2933,  2271,  1006,  6948,  1007,  2003,\n",
       "          1037, 11888, 20187,  1998, 11311,  1011, 19872,  4295,  2008,\n",
       "         13531,  1996,  3096,  1010, 10063,  1010,  2606,  1010,  1998,\n",
       "         14163, 27199, 24972,  1012,  1031,  1015,  1033,  2009,  2003,\n",
       "          7356,  2011, 26572, 20028,  1010,  4257,  1011,  9370,  1010,\n",
       "         10482,  3401,  3560,  6643, 14289,  4244,  1998, 28487,  2007,\n",
       "         15241,  2075,  1010,  2128,  4588,  8898,  1010,  2986,  2317,\n",
       "          4094,  1006, 15536,  3600,  3511,  1005,  1055,  2358,  4360,\n",
       "          2063,  1007,  1010,  4141, 12473, 12759,  2398,  1010, 23951,\n",
       "         11137, 12150,  1998, 27323,  1010,  8260,  1010, 15099,  2896,\n",
       "          3456,  1998,  8700, 14163, 13186,  2050,  1012,  1031,  1016,\n",
       "          1033,  2348,  2045,  2003,  1037,  5041,  6612,  2846,  1997,\n",
       "          6948, 24491,  2015,  1010,  1996,  3096,  1998,  8700, 17790,\n",
       "          3961,  2004,  1996,  2350,  4573,  1997,  6624,  1012,  1031,\n",
       "          1017,  1033,  1996,  3426,  2003,  4242,  1010,  2021,  2009,\n",
       "          2003,  2245,  2000,  2022,  1996,  2765,  1997,  2019,  8285,\n",
       "          5714, 23041,  2063,  2832,  2007,  2019,  4242,  3988,  9495,\n",
       "          1012,  2045,  2003,  2053,  9526,  1010,  2021,  2116,  2367,\n",
       "         20992,  1998,  8853,  2031,  2042,  2109,  1999,  4073,  2000,\n",
       "          2491,  1996,  8030,  1012,  1996,  2744,  5622,  8661,  9314,\n",
       "          4668,  1006,  5622,  8661,  9314, 17259,  2030,  5622,  8661,\n",
       "          9314,  4649,  3258,  1007,  5218,  2000,  1037,  4649,  3258,\n",
       "          1997,  2714,  2030,  7235,  2010, 14399,  8988, 12898, 12863,\n",
       "          1998,  6612,  3311,  2000,  5622,  8661,  2933,  2271,  1006,\n",
       "          1045,  1012,  1041,  1012,  1010,  2019,  2181,  2029, 12950,\n",
       "          5622,  8661,  2933,  2271,  1010,  2119,  2000,  1996,  6248,\n",
       "          3239,  1998,  2104,  1037, 24635,  1007,  1012,  1031,  1018,\n",
       "          1033,  1031,  1019,  1033,  2823, 11394,  4475,  2030,  3056,\n",
       "         20992,  2064,  3426,  1037,  5622,  8661,  9314,  4668,  1012,\n",
       "          1031,  1018,  1033,  2027,  2064,  2036,  5258,  1999,  2523,\n",
       "          2007, 22160,  2102,  6431,  3677,  4295,  1012,  1031,  1018,\n",
       "          1033,  1031,  1020,  1033,  1024, 24398,  8417,  1015,  5579,\n",
       "          1015,  1012,  1015,  2609,  1015,  1012,  1016,  5418,  1015,\n",
       "          1012,  1017, 17702,  8715,  2015,  1016,  5751,  1998,  8030,\n",
       "          1016,  1012,  1015,  3096,  1016,  1012,  1016, 14163, 27199,\n",
       "         24972,  1017,  5320,  1018, 26835, 19009,  1019, 11616,  1019,\n",
       "          1012,  1015,  3096,  1019,  1012,  1016,  2677,  1019,  1012,\n",
       "          1017, 11658, 11616,  1019,  1012,  1018,  2010, 14399,  8988,\n",
       "          6779,  1020,  3949,  1020,  1012,  1015,  3096,  1020,  1012,\n",
       "          1016,  2677,  1021,  4013, 26745,  6190,  1022,  4958,  5178,\n",
       "          4328,  6779,  1023,  2381,  2184,  2470,  2340,  3964,  2260,\n",
       "          7604,  2410,  6327,  6971,  5579,  5622,  8661,  2933,  2271,\n",
       "         22520,  2024,  2061,  2170,  2138,  1997,  2037,  1000,  5622,\n",
       "          8661,  1011,  2066,  1000,  3311,  1031,  1021,  1033,  1998,\n",
       "          2064,  2022,  6219,  2011,  1996,  2609,  2027,  9125,  1010,\n",
       "          2030,  2011,  2037, 19476,  1012,  2609,  5622,  8661,  2933,\n",
       "          2271,  2089,  2022, 20427,  2004, 12473, 14163, 13186,  2389,\n",
       "          2030,  3013, 17191,  9972,  1012,  3013, 17191,  3596,  2024,\n",
       "          2216, 12473,  1996,  3096,  1010, 21065,  1010,  1998, 10063,\n",
       "          1012,  1031,  1022,  1033,  1031,  1023,  1033,  1031,  2184,\n",
       "          1033, 14163, 13186,  2389,  3596,  2024,  2216, 12473,  1996,\n",
       "         14834,  1997,  1996,  3806, 13181, 18447, 19126, 12859,  1006,\n",
       "          2677,  1010,  6887,  5649, 26807,  1010,  9686,  7361,  3270,\n",
       "         12349,  1010,  4308,  1010,  2019,  2271,  1007,  1010,  2474,\n",
       "         18143,  2595,  1010,  1998,  2060, 14163, 13186,   102])>,\n",
       "  'attention_mask': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
       "  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1])>},\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 4s 2s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 5.1896e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x208b2d99108>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline training\n",
    "model.fit(train_dataset.shuffle(1000).batch(2), epochs=1, batch_size=16, validation_data=val_dataset.shuffle(1000).batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-9d01dfc21c99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2281\u001b[0m             )\n\u001b[0;32m   2282\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2283\u001b[1;33m             \u001b[1;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2284\u001b[0m             \u001b[1;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m         )\n",
      "\u001b[1;31mAssertionError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "pipe.tokenizer(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[-0.12484591],\n",
       "       [-0.05797866],\n",
       "       [-0.10238954],\n",
       "       ...,\n",
       "       [-0.1679904 ],\n",
       "       [-0.18195675],\n",
       "       [-0.09280173]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(15, 768), b.shape=(768, 768), m=15, n=768, k=768 [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d846a095e40f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfm_pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'distilbert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'distilbert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextClassificationPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\auto\\modeling_tf_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m             return TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING[type(config)].from_pretrained(\n\u001b[1;32m-> 1150\u001b[1;33m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m             )\n\u001b[0;32m   1152\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_pytorch_checkpoint_in_tf2_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_missing_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdummy_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build the network with dummy inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Error retrieving file {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m         )\n\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m         )\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    363\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             \u001b[0mlayer_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, training)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \"\"\"\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# Self-Attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0msa_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0msa_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msa_output\u001b[0m  \u001b[1;31m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions, training)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_heads\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_lin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, n_heads, q_length, dim_per_head)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_lin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_lin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         dtype=self._compute_dtype_object)\n\u001b[0m\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\ops\\core.py\u001b[0m in \u001b[0;36mdense\u001b[1;34m(inputs, kernel, bias, activation, dtype)\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[1;31m# Broadcast kernel to inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandard_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# Reshape the output back to the original ndim of the input.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mtensordot\u001b[1;34m(a, b, axes, name)\u001b[0m\n\u001b[0;32m   4516\u001b[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001b[0;32m   4517\u001b[0m         b, b_axes, True)\n\u001b[1;32m-> 4518\u001b[1;33m     \u001b[0mab_matmul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_reshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4519\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_free_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_free_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4520\u001b[0m       if (ab_matmul.get_shape().is_fully_defined() and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   3252\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3253\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 3254\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   3255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5622\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5623\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5624\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5625\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5626\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models2\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(15, 768), b.shape=(768, 768), m=15, n=768, k=768 [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, pipeline as tfm_pipeline\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5203559398651123}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"asddd ssss \", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[  101,  2004, 14141,  2094,  7020,  4757,  4241,  4502,   102]])>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"asddd ssss \",\n",
    "                               truncation=True,\n",
    "                               padding=True,\n",
    "                               return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = tfm_pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, framework=\"tf\")\n",
    "score = pipeline.model.predict(pipeline.tokenizer.encode(\"asddd ssss \",\n",
    "                               truncation=True,\n",
    "                               padding=True,\n",
    "                               return_tensors=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.nn import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5203559398651123}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"asddd ssss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='./temp', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.02770592, -0.05376299]], dtype=float32),)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokenizer.encode(\"asddd ssss Dupa\",\n",
    "                               truncation=True,\n",
    "                               padding=True,\n",
    "                               return_tensors=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save_pretrained(\"./temp_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./saved_models were not used when initializing TFDistilBertForSequenceClassification: ['dropout_39']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./saved_models and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "save_directory = \"./saved_models\" # change this to your preferred location\n",
    "\n",
    "model.save_pretrained(\"./saved_models\")\n",
    "tokenizer.save_pretrained(\"./saved_models\")\n",
    "\n",
    "loaded_tokenizer = DistilBertTokenizer.from_pretrained(save_directory)\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='./saved_models', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = tf.keras.models.load_model(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-e0252d67b2f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./temp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \"\"\"\n\u001b[0;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1979\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dermclass_models\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    122\u001b[0m         not isinstance(model, sequential.Sequential)):\n\u001b[0;32m    123\u001b[0m       raise NotImplementedError(\n\u001b[1;32m--> 124\u001b[1;33m           \u001b[1;34m'Saving the model to HDF5 format requires the model to be a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m           \u001b[1;34m'Functional model or a Sequential model. It does not work for '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m           \u001b[1;34m'subclassed models, because such models are defined via the body of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model.save(\"./temp\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(tokenizer, \"./test.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-b4bea39108cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m predict_input = loaded_tokenizer.encode(test_text,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                  \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                  return_tensors=\"tf\")\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "predict_input = loaded_tokenizer.encode(test_text,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\")\n",
    "\n",
    "output = loaded_model(predict_input)[0]\n",
    "\n",
    "prediction_value = tf.argmax(output, axis=1).numpy()[0]\n",
    "prediction_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
